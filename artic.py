# -*- coding: utf-8 -*-
"""ARTIC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12pKW9UBgeAJj26COXMsY06QOO4SWM8lH

# **define variables and set parameters**
"""

import os
import re
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import efficientnet
from tensorflow.keras.layers import TextVectorization


seed = 111
np.random.seed(seed)
tf.random.set_seed(seed)

"""# **Pre Processing**"""

!pip install pyarabic
import nltk
import re
from nltk.stem.isri import ISRIStemmer
import pandas as pd
import pyarabic.araby as araby

def normalize(sent):

     sent=re.sub('#', '', sent)
     sent=re.sub('_', '', sent)
     sent=re.sub( r'[a-zA-Z0-9]', '', sent)
     sent=re.sub('-', '', sent)
     sent=re.sub('"', '', sent)
     sent=re.sub("'", '', sent)
     sent=re.sub("\.", '', sent)
     sent=re.sub("-", '', sent)
     sent=re.sub("\*", '', sent)
     sent=re.sub("@", '', sent)
     sent=re.sub("\(", '', sent)
     sent=re.sub("\)", '', sent)

     sent=re.sub("[࿐✿❃˺↓]", '', sent)




     sent=araby.strip_tashkeel(sent)
     sent=araby.strip_tatweel(sent)
     word_list=sent.split(' ')
     processed_word_list = []
     for word in word_list:
       word=st.norm(word,3)

       suffix = 'ي'
       if (word.endswith(suffix)):
           word = word[:-1] +'ى'


       suffix = 'ة'
       if (word.endswith(suffix)):
           word = word[:-1] +'ه'
       pref1='ال'

       prefN='ا'
       prefWaw='و'
       if (word.startswith(pref1)  ):
           #print('found al')
           word=word[2:]
       word = re.sub("[إأٱآا]", prefN,word)
       word = re.sub("[وؤ]", prefWaw,word)

      # word=normalizeHashMentionNumber(word)
       processed_word_list.append(word)


     sent=' '.join(processed_word_list)

     return (sent)
def remove_stopwords(sent,stopWordList):
        word_list=sent.split(' ')
        processed_word_list = []
        for word in word_list:
            word = word.lower() # in case they arenet all lower cased
            if word not in stopWordList:
                processed_word_list.append(word)

        sent=' '.join(processed_word_list)
        return sent

def remove_shortWords(sent):
        word_list=sent.split(' ')
        processed_word_list = []
        for word in word_list:
            if len(word) > 2:
                processed_word_list.append(word)

        sent=' '.join(processed_word_list)
        return sent

def stemOfString(sent):
        word_list=sent.split(' ')
        processed_word_list = []
        for word in word_list:
                word=st.stem(word)
                if (len(word)>2):
                 processed_word_list.append(word)

        sent=' '.join(processed_word_list)
        return sent

def preProcess(text):
    text=text.strip()
    text=normalize(text)
    #print("After Normalize")
    #print(text)


    #text=remove_stopwords(text,stopWordList)
    #print("After remove_stopwords")

    #print(text)

    text=stemOfString(text)
    #print("After StemOfString")

    #print(text)

    text=remove_shortWords(text)
    #print("After remove_shortWords")

    #print(text)
    text=text.strip()

    return text
st = ISRIStemmer()

IMAGES_PATH ="/Dataset"


# Desired image dimensions
IMAGE_SIZE = (300, 300)

# Vocabulary size
VOCAB_SIZE = 10000

# Fixed length allowed for any sequence
SEQ_LENGTH = 25

# Dimension for the image embeddings and token embeddings
EMBED_DIM = 512#1024

# Hidden layer size in feed forward network inside transformer
FF_DIM = 1024#512                        dense_dim
# Other training parameters
BATCH_SIZE =64# , 128,32,512
EPOCHS = 30#15#10

import pandas as pd

AUTOTUNE = tf.data.AUTOTUNE

filename='/Dataset/arabic fliker8k text data/Flickr8k.arabic.full.xlsx'
df=pd.read_excel(filename)
"""image_name=list(df['image_name'])
number=list(df['number'])
caption=list(df['caption'])"""

"""# **Preparing the dataset**"""

def load_captions_data(df):
    image_name=list(df['image_name'])
    number=list(df['number'])
    caption1=list(df['caption'])


    caption_mapping = {}
    text_data = []
    images_to_skip = set()

    for i in range(len(image_name)):

        img_name = os.path.join(IMAGES_PATH, image_name[i].strip())
        caption=caption1[i]
        # We will remove caption that are either too short to too long
        tokens = caption.strip().split()

        if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:
            images_to_skip.add(img_name)
            continue

        if img_name.endswith("jpg") and img_name not in images_to_skip:


            caption=preProcess(caption) # Preprocess
            caption = "<start> " + caption.strip() + " <end>" # We will add a start and an end token to each caption

            text_data.append(caption)

            if img_name in caption_mapping:
                caption_mapping[img_name].append(caption)
            else:
                caption_mapping[img_name] = [caption]

    for img_name in images_to_skip:
        if img_name in caption_mapping:
            del caption_mapping[img_name]

    return caption_mapping, text_data
captions_mapping, text_data = load_captions_data(df)


def train_val_split(caption_data, train_size=.8, shuffle=False):

    # 1. Get the list of all image names
    all_images = list(caption_data.keys())

    # 2. Shuffle if necessary
    if shuffle:
        np.random.shuffle(all_images)

    # 3. Split into training and validation sets
    #train_size = int(len(caption_data) * train_size)
    print('final size data:',len(all_images))
    train_size = len(all_images)-2000
    valid_size=train_size+1000
    test_size=valid_size+1000
    training_data = {
        img_name: caption_data[img_name] for img_name in all_images[:train_size]
    }
    validation_data = {
        img_name: caption_data[img_name] for img_name in all_images[train_size:valid_size]
    }

    testing_data = {
        img_name: caption_data[img_name] for img_name in all_images[valid_size:test_size]
    }


    # 4. Return the splits
    return training_data, validation_data ,testing_data


# Load the dataset
captions_mapping, text_data = load_captions_data(df)

# Split the dataset into training and validation sets
train_data, valid_data,test_data = train_val_split(captions_mapping)
print("Number of training samples: ", len(train_data))
print("Number of validation samples: ", len(valid_data))
print("Number of test samples: ", len(test_data))

"""# **Vectorizing the text data**

"""

def custom_standardization(input_string):
    lowercase = tf.strings.lower(input_string)
    return tf.strings.regex_replace(lowercase, "[%s]" % re.escape(strip_chars), "")


strip_chars = "!\"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"
strip_chars = strip_chars.replace("<", "")
strip_chars = strip_chars.replace(">", "")

vectorization = TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_mode="int",
    output_sequence_length=SEQ_LENGTH,
    standardize=custom_standardization,
)
vectorization.adapt(text_data)

# Data augmentation for image data
image_augmentation = keras.Sequential(
    [
      layers.RandomFlip("horizontal"),
      layers.experimental.preprocessing.RandomContrast(factor=(0.05, 0.15)),
    	layers.experimental.preprocessing.RandomTranslation(height_factor=(-0.10, 0.10), width_factor=(-0.10, 0.10)),
	    layers.experimental.preprocessing.RandomZoom(height_factor=(-0.10, 0.10), width_factor=(-0.10, 0.10)),
    	layers.experimental.preprocessing.RandomRotation(factor=(-0.10, 0.10))
    ]
)

def custom_standardization(input_string):
    lowercase = tf.strings.lower(input_string)
    return tf.strings.regex_replace(lowercase, "[%s]" % re.escape(strip_chars), "")


strip_chars = "!\"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"

strip_chars = strip_chars.replace("<", "")
strip_chars = strip_chars.replace(">", "")
strip_chars = strip_chars.replace("ة", "ه")
strip_chars = strip_chars.replace("أ", "ا")

vectorization = TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_mode="int",
    output_sequence_length=SEQ_LENGTH,
    standardize=custom_standardization,
)
vectorization.adapt(text_data)

# Data augmentation for image data
image_augmentation = keras.Sequential(
    [
      layers.RandomFlip("horizontal"),
      layers.experimental.preprocessing.RandomContrast(factor=(0.05, 0.15)),
    	layers.experimental.preprocessing.RandomTranslation(height_factor=(-0.10, 0.10), width_factor=(-0.10, 0.10)),
	    layers.experimental.preprocessing.RandomZoom(height_factor=(-0.10, 0.10), width_factor=(-0.10, 0.10)),
    	layers.experimental.preprocessing.RandomRotation(factor=(-0.10, 0.10))
    ]
)

"""# **Generating pairs of (images and corresponding captions)**

"""

def decode_and_resize(img_path):
    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, IMAGE_SIZE)
    img = tf.image.convert_image_dtype(img, tf.float32)
    return img


def process_input(img_path, captions):
    return decode_and_resize(img_path), vectorization(captions)


def make_dataset(images, captions):
    dataset = tf.data.Dataset.from_tensor_slices((images, captions))
    dataset = dataset.shuffle(len(images))
    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)
    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)

    return dataset


# Pass the list of images and the list of corresponding captions
train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))

valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))
test_dataset = make_dataset(list(test_data.keys()), list(test_data.values()))

"""# **Building the model**




"""

from tensorflow.keras.applications import efficientnet, resnet50, vgg16,vgg19,efficientnet_v2

def get_cnn_model():
    base_model =  resnet50.ResNet50(
        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights="imagenet",
    )
    # We freeze our feature extractor
    base_model.trainable = False
    base_model_out = base_model.output
    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)
    cnn_model = keras.models.Model(base_model.input, base_model_out)
    return cnn_model

class TransformerEncoderBlock(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.0
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.dense_1 = layers.Dense(embed_dim, activation="relu")

    def call(self, inputs, training, mask=None):
        inputs = self.layernorm_1(inputs)
        inputs = self.dense_1(inputs)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=None,
            training=training,
        )
        out_1 = self.layernorm_2(inputs + attention_output_1)
        return out_1


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_tokens = embedded_tokens * self.embed_scale
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)


class TransformerDecoderBlock(layers.Layer):
    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.3
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.2
        )
        self.ffn_layer_1 = layers.Dense(ff_dim, activation="relu")
        self.ffn_layer_2 = layers.Dense(embed_dim)

        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()

        self.embedding = PositionalEmbedding(
            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE
        )
        self.out = layers.Dense(VOCAB_SIZE, activation="softmax")

        self.dropout_1 = layers.Dropout(0.3)#0.1
        self.dropout_2 = layers.Dropout(0.5)#0.1
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, training, mask=None):
        inputs = self.embedding(inputs)
        causal_mask = self.get_causal_attention_mask(inputs)

        if mask is not None:
            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)
            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
            combined_mask = tf.minimum(combined_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=combined_mask,
            training=training,
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
            training=training,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        ffn_out = self.ffn_layer_1(out_2)
        ffn_out = self.dropout_1(ffn_out, training=training)
        ffn_out = self.ffn_layer_2(ffn_out)

        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)
        ffn_out = self.dropout_2(ffn_out, training=training)
        preds = self.out(ffn_out)
        return preds

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)


class ImageCaptioningModel(keras.Model):
    def __init__(
        self, cnn_model, encoder, decoder, num_captions_per_image=3, image_aug=None,
    ):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.acc_tracker = keras.metrics.Mean(name="accuracy")
        self.num_captions_per_image = num_captions_per_image
        self.image_aug = image_aug

    def calculate_loss(self, y_true, y_pred, mask):
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)

    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        accuracy = tf.math.logical_and(mask, accuracy)
        accuracy = tf.cast(accuracy, dtype=tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)

    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):
        encoder_out = self.encoder(img_embed, training=training)
        batch_seq_inp = batch_seq[:, :-1]
        batch_seq_true = batch_seq[:, 1:]
        mask = tf.math.not_equal(batch_seq_true, 0)
        batch_seq_pred = self.decoder(
            batch_seq_inp, encoder_out, training=training, mask=mask
        )
        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)
        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)
        return loss, acc

    def train_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        if self.image_aug:
            batch_img = self.image_aug(batch_img)

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            with tf.GradientTape() as tape:
                loss, acc = self._compute_caption_loss_and_acc(
                    img_embed, batch_seq[:, i, :], training=True
                )

                # 3. Update loss and accuracy
                batch_loss += loss
                batch_acc += acc

            # 4. Get the list of all the trainable weights
            train_vars = (
                self.encoder.trainable_variables + self.decoder.trainable_variables
            )

            # 5. Get the gradients
            grads = tape.gradient(loss, train_vars)

            # 6. Update the trainable weights
            self.optimizer.apply_gradients(zip(grads, train_vars))

        # 7. Update the trackers
        batch_acc /= float(self.num_captions_per_image)
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 8. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    def test_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            loss, acc = self._compute_caption_loss_and_acc(
                img_embed, batch_seq[:, i, :], training=False
            )

            # 3. Update batch loss and batch accuracy
            batch_loss += loss
            batch_acc += acc

        batch_acc /= float(self.num_captions_per_image)

        # 4. Update the trackers
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 5. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    @property
    def metrics(self):
        # We need to list our metrics here so the `reset_states()` can be
        # called automatically.
        return [self.loss_tracker, self.acc_tracker]


cnn_model = get_cnn_model()
encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=5)
decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=6)
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,
)

"""# **Model training**"""

# Define the loss function
cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction="none")

# EarlyStopping criteria
early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)


# Learning Rate Scheduler for the optimizer
class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, post_warmup_learning_rate, warmup_steps):
        super().__init__()
        self.post_warmup_learning_rate = post_warmup_learning_rate
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        global_step = tf.cast(step, tf.float32)
        warmup_steps = tf.cast(self.warmup_steps, tf.float32)
        warmup_progress = global_step / warmup_steps
        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress
        return tf.cond(
            global_step < warmup_steps,
            lambda: warmup_learning_rate,
            lambda: self.post_warmup_learning_rate,
        )


# Create a learning rate schedule
num_train_steps = len(train_dataset) * EPOCHS
num_warmup_steps = num_train_steps //30 #15
lr_schedule = LRSchedule(post_warmup_learning_rate= 1e-4, warmup_steps=num_warmup_steps)# 1e-4


#caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy
caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy

# Fit the model
caption_model.fit(train_dataset,epochs=EPOCHS,validation_data=valid_dataset,callbacks=[early_stopping])

# Compute definitive metrics on train/valid set
train_metrics = caption_model.evaluate(train_dataset, batch_size=BATCH_SIZE)
valid_metrics = caption_model.evaluate(valid_dataset, batch_size=BATCH_SIZE)

test_metrics = caption_model.evaluate(test_dataset, batch_size=BATCH_SIZE)

print("Train Loss = %.4f - Train Accuracy = %.4f" % (train_metrics[0], train_metrics[1]))
print("Valid Loss = %.4f - Valid Accuracy = %.4f" % (valid_metrics[0], valid_metrics[1]))

print("Test Loss = %.4f - Test Accuracy = %.4f" % (test_metrics[0], test_metrics[1]))

"""**saved and load model**"""

caption_model.save_weights('/saved model/ResNet50/')
print('saved done!')
caption_model.load_weights('/saved model/ResNet50/')



"""# **prediction testing belue**"""

from tqdm import tqdm
"""
vocab = vectorization.get_vocabulary()
index_lookup = dict(zip(range(len(vocab)), vocab))
max_decoded_sentence_length = SEQ_LENGTH - 1
valid_images = list(valid_data.keys())
"""

def generate_caption_noimg(data12):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in tqdm(data12):

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      for i in range(max_decoded_sentence_length):
          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token = index_lookup[sampled_token_index]
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)

def chop_start_end(cpt):
    if cpt.startswith('<start>') and cpt.endswith('<end>'):
        cpt = ' '.join(cpt.split()[1:-1])
    return cpt

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def BLEU_corpus(actual_captions, generated_captions):
    list_of_references = []
    for images_captions in actual_captions:
        list_of_references.append([chop_start_end(cpt).split() for cpt in images_captions])
    hypotheses = [chop_start_end(cpt).split() for cpt in generated_captions]
    b1 = corpus_bleu(list_of_references, hypotheses, weights=(1, 0, 0, 0))
    b2 = corpus_bleu(list_of_references, hypotheses, weights=(0.5, 0.5, 0, 0))
    b3 = corpus_bleu(list_of_references, hypotheses, weights=(0.333, 0.333, 0.333, 0))
    b4 = corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))
    return b1, b2, b3, b4

def plot_bleu_bar_graph(bleu_train, bleu_test):
    n_groups = 4
    #bleu_train = (b1_train,b2_train,b3_train,b4_train)
    #bleu_test = (b1_test,b2_test,b3_test,b4_test)
    # create plot
    fig, ax = plt.subplots()
    index = np.arange(n_groups)
    bar_width = 0.3
    opacity = 0.8

    rects1 = plt.bar(index, bleu_train, bar_width,
                     alpha=opacity,
                     color='b',
                     label='train',
                     zorder=3)

    rects2 = plt.bar(index + bar_width, bleu_test, bar_width,
                     alpha=opacity,
                     color='r',
                     label='test',
                     zorder=3)

    plt.xlabel('BLEU')
    plt.ylabel('score')
    #plt.title('Scores')
    plt.xticks(index + bar_width, ('1', '2', '3', '4'))
    plt.legend()
    plt.tight_layout()
    plt.grid(True, axis='y')
    plt.savefig("BLEU_bar.png", dpi=200)
    plt.show()

testing_data=test_data
#test_dataset
print("Number of testing samples: ", len(testing_data))

all_gen_test=generate_caption_noimg(list(testing_data.keys()))
all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])
all_gen_test
for i in range(len(all_gen_test)):
  all_gen_test[i]='<start> '+ all_gen_test[i]+ ' <end>'
b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, all_gen_test)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

"""# **beam search**"""

#sampled_token_index11=[]
def generate_caption_noimg(data12,caption_model):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in data12:

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      start_token = "<start>"
      end_token = "<end>"
      #start = [wordtoix[start_token]]

      #start_word = [[start, 0.0]]


      for i in (range(max_decoded_sentence_length)):

          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token_index11.append(predictions[0, i, :])
          #print('sampled_token_index:',sampled_token_index)
          sampled_token = index_lookup[sampled_token_index]
          #print('sampled_token:',sampled_token)
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)
from math import log
from numpy import array
from numpy import argmax

# beam search
def beam_search_decoder(data, k):
	sequences = [[list(), 0.0]]
	# walk over each step in sequence
	for row in data:
		all_candidates = list()
		# expand each current candidate
		for i in range(len(sequences)):
			seq, score = sequences[i]
			for j in range(len(row)):
				candidate = [seq + [j], score - log(row[j])]
				all_candidates.append(candidate)


		# order all candidates by score
		ordered = sorted(all_candidates, key=lambda tup:tup[1])
		# select k best
		sequences = ordered[:k]
	return sequences

# define a sequence of 10 words over a vocab of 5 words

#greedy_result=(generate_caption_noimg([list(testing_data.keys())[3],caption_model))
#beam_search_result=beam_search_decoder(np.array(sampled_token_index11),5)

all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])

sampled_token_index11=[]
final_caption_using_beam_search_and_bleu_score=[]
list_test=list(testing_data.keys())

for k in tqdm(range (len(list_test))):
  greedy_result=(generate_caption_noimg([list_test[k]],caption_model))
  #print('greedy_result:',greedy_result)
  beam_search_result=beam_search_decoder(np.array(sampled_token_index11),10)
  #print(beam_search_result)


  caption_gen_test=[]
  for i in beam_search_result:
    text=''
    for j in i[0]:
      if j!=4:
        text=text+' '+index_lookup[j]
    #print('<start> '+text+ ' <end>')
    caption_gen_test.append('<start> '+text+ ' <end>')
  #print(caption_gen_test)
  b1=[]

  for caption in caption_gen_test:
    b1_test,b2_test,b3_test,b4_test = BLEU_corpus([all_true_test[k]], [caption])
    b1.append(b1_test)

  final_caption_using_beam_search_and_bleu_score.append(caption_gen_test[b1.index(max(b1))])
  #print(final_caption_using_beam_search_and_bleu_score)
    #print('##########################################################')

  sampled_token_index11=[]

for i in range(len(final_caption_using_beam_search_and_bleu_score)):
  final_caption_using_beam_search_and_bleu_score[i]=str(final_caption_using_beam_search_and_bleu_score[i]).replace('<end>','').strip()+' <end>'

import re
for i in range(len(final_caption_using_beam_search_and_bleu_score)):

  final_caption_using_beam_search_and_bleu_score[i]=re.sub(' +', ' ',final_caption_using_beam_search_and_bleu_score[i])

b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, final_caption_using_beam_search_and_bleu_score)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

data=pd.DataFrame(zip(all_true_test,all_gen_test,final_caption_using_beam_search_and_bleu_score),columns=['all_true_test','all_gen_test','final_caption_using_beam_search_with_k_10_and_bleu_score'])

data.to_excel('/save excels/ResNet50.xlsx')

"""scoers"""

!pip install pycocoevalcap
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice
import json


def bleu(gts,res):
    scorer = Bleu(n=4)
    # scorer += (hypo[0], ref1)   # hypo[0] = 'word1 word2 word3 ...'
    #                                 # ref = ['word1 word2 word3 ...', 'word1 word2 word3 ...']
    score, scores = scorer.compute_score(gts, res)
    print('belu = %s' % score)
    return score

def cider(gts,res):
    scorer = Cider()
    # scorer += (hypo[0], ref1)
    (score, scores) = scorer.compute_score(gts, res)
    print('cider = %s' % score)
    return score

def meteor(gts,res):
    scorer = Meteor()
    score, scores = scorer.compute_score(gts, res)
    print('meter = %s' % score)
    return score

def rouge(gts,res):
    scorer = Rouge()
    score, scores = scorer.compute_score(gts, res)
    print('rouge = %s' % score)
    return score

def spice(gts,res):
    scorer = Spice()
    score, scores = scorer.compute_score(gts, res)
    print('spice = %s' % score)
    return score

def main(gts,res):
    b1,b2,b3,b4=bleu(gts,res)
    cid=cider(gts,res)
    met=meteor(gts,res)
    rou=rouge(gts,res)
    spi=spice(gts,res)
    return b1,b2,b3,b4,cid,met,rou,spi

#score ith greedy search

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(all_gen_test[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)

#score with beam search k=10

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(final_caption_using_beam_search_and_bleu_score[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)

def get_cnn_model():
    base_model =  resnet.ResNet121(
        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights="imagenet",
    )
    # We freeze our feature extractor
    base_model.trainable = False
    base_model_out = base_model.output
    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)
    cnn_model = keras.models.Model(base_model.input, base_model_out)
    return cnn_model

class TransformerEncoderBlock(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.0
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.dense_1 = layers.Dense(embed_dim, activation="relu")

    def call(self, inputs, training, mask=None):
        inputs = self.layernorm_1(inputs)
        inputs = self.dense_1(inputs)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=None,
            training=training,
        )
        out_1 = self.layernorm_2(inputs + attention_output_1)
        return out_1


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_tokens = embedded_tokens * self.embed_scale
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)


class TransformerDecoderBlock(layers.Layer):
    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.3
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.2
        )
        self.ffn_layer_1 = layers.Dense(ff_dim, activation="relu")
        self.ffn_layer_2 = layers.Dense(embed_dim)

        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()

        self.embedding = PositionalEmbedding(
            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE
        )
        self.out = layers.Dense(VOCAB_SIZE, activation="softmax")

        self.dropout_1 = layers.Dropout(0.3)#0.1
        self.dropout_2 = layers.Dropout(0.5)#0.1
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, training, mask=None):
        inputs = self.embedding(inputs)
        causal_mask = self.get_causal_attention_mask(inputs)

        if mask is not None:
            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)
            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
            combined_mask = tf.minimum(combined_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=combined_mask,
            training=training,
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
            training=training,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        ffn_out = self.ffn_layer_1(out_2)
        ffn_out = self.dropout_1(ffn_out, training=training)
        ffn_out = self.ffn_layer_2(ffn_out)

        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)
        ffn_out = self.dropout_2(ffn_out, training=training)
        preds = self.out(ffn_out)
        return preds

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)


class ImageCaptioningModel(keras.Model):
    def __init__(
        self, cnn_model, encoder, decoder, num_captions_per_image=3, image_aug=None,
    ):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.acc_tracker = keras.metrics.Mean(name="accuracy")
        self.num_captions_per_image = num_captions_per_image
        self.image_aug = image_aug

    def calculate_loss(self, y_true, y_pred, mask):
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)

    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        accuracy = tf.math.logical_and(mask, accuracy)
        accuracy = tf.cast(accuracy, dtype=tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)

    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):
        encoder_out = self.encoder(img_embed, training=training)
        batch_seq_inp = batch_seq[:, :-1]
        batch_seq_true = batch_seq[:, 1:]
        mask = tf.math.not_equal(batch_seq_true, 0)
        batch_seq_pred = self.decoder(
            batch_seq_inp, encoder_out, training=training, mask=mask
        )
        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)
        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)
        return loss, acc

    def train_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        if self.image_aug:
            batch_img = self.image_aug(batch_img)

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            with tf.GradientTape() as tape:
                loss, acc = self._compute_caption_loss_and_acc(
                    img_embed, batch_seq[:, i, :], training=True
                )

                # 3. Update loss and accuracy
                batch_loss += loss
                batch_acc += acc

            # 4. Get the list of all the trainable weights
            train_vars = (
                self.encoder.trainable_variables + self.decoder.trainable_variables
            )

            # 5. Get the gradients
            grads = tape.gradient(loss, train_vars)

            # 6. Update the trainable weights
            self.optimizer.apply_gradients(zip(grads, train_vars))

        # 7. Update the trackers
        batch_acc /= float(self.num_captions_per_image)
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 8. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    def test_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            loss, acc = self._compute_caption_loss_and_acc(
                img_embed, batch_seq[:, i, :], training=False
            )

            # 3. Update batch loss and batch accuracy
            batch_loss += loss
            batch_acc += acc

        batch_acc /= float(self.num_captions_per_image)

        # 4. Update the trackers
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 5. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    @property
    def metrics(self):
        # We need to list our metrics here so the `reset_states()` can be
        # called automatically.
        return [self.loss_tracker, self.acc_tracker]


cnn_model = get_cnn_model()
encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=5)
decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=6)
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,
)



# Define the loss function
cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction="none")

# EarlyStopping criteria
early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)


# Learning Rate Scheduler for the optimizer
class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, post_warmup_learning_rate, warmup_steps):
        super().__init__()
        self.post_warmup_learning_rate = post_warmup_learning_rate
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        global_step = tf.cast(step, tf.float32)
        warmup_steps = tf.cast(self.warmup_steps, tf.float32)
        warmup_progress = global_step / warmup_steps
        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress
        return tf.cond(
            global_step < warmup_steps,
            lambda: warmup_learning_rate,
            lambda: self.post_warmup_learning_rate,
        )


# Create a learning rate schedule
num_train_steps = len(train_dataset) * EPOCHS
num_warmup_steps = num_train_steps //15 #15
lr_schedule = LRSchedule(post_warmup_learning_rate= 1e-4, warmup_steps=num_warmup_steps)# 1e-4

# Compile the model
#caption_model.compile(optimizer=keras.optimizers.Adamax(lr_schedule), loss=cross_entropy)#loss=cross_entropy

#caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy
caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy

# Fit the model
caption_model.fit(train_dataset,epochs=EPOCHS,validation_data=valid_dataset,callbacks=[early_stopping])



# Compute definitive metrics on train/valid set
train_metrics = caption_model.evaluate(train_dataset, batch_size=BATCH_SIZE)
valid_metrics = caption_model.evaluate(valid_dataset, batch_size=BATCH_SIZE)

test_metrics = caption_model.evaluate(test_dataset, batch_size=BATCH_SIZE)

print("Train Loss = %.4f - Train Accuracy = %.4f" % (train_metrics[0], train_metrics[1]))
print("Valid Loss = %.4f - Valid Accuracy = %.4f" % (valid_metrics[0], valid_metrics[1]))

print("Test Loss = %.4f - Test Accuracy = %.4f" % (test_metrics[0], test_metrics[1]))

caption_model.save_weights('/saved model/ResNet121/')
print('saved done!')
caption_model.load_weights('/saved model /ResNet121/')

"""# **prediction testing belue**"""

from tqdm import tqdm
"""
vocab = vectorization.get_vocabulary()
index_lookup = dict(zip(range(len(vocab)), vocab))
max_decoded_sentence_length = SEQ_LENGTH - 1
valid_images = list(valid_data.keys())
"""

def generate_caption_noimg(data12):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in tqdm(data12):

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      for i in range(max_decoded_sentence_length):
          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token = index_lookup[sampled_token_index]
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)

def chop_start_end(cpt):
    if cpt.startswith('<start>') and cpt.endswith('<end>'):
        cpt = ' '.join(cpt.split()[1:-1])
    return cpt

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def BLEU_corpus(actual_captions, generated_captions):
    list_of_references = []
    for images_captions in actual_captions:
        list_of_references.append([chop_start_end(cpt).split() for cpt in images_captions])
    hypotheses = [chop_start_end(cpt).split() for cpt in generated_captions]
    b1 = corpus_bleu(list_of_references, hypotheses, weights=(1, 0, 0, 0))
    b2 = corpus_bleu(list_of_references, hypotheses, weights=(0.5, 0.5, 0, 0))
    b3 = corpus_bleu(list_of_references, hypotheses, weights=(0.333, 0.333, 0.333, 0))
    b4 = corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))
    return b1, b2, b3, b4

def plot_bleu_bar_graph(bleu_train, bleu_test):
    n_groups = 4
    #bleu_train = (b1_train,b2_train,b3_train,b4_train)
    #bleu_test = (b1_test,b2_test,b3_test,b4_test)
    # create plot
    fig, ax = plt.subplots()
    index = np.arange(n_groups)
    bar_width = 0.3
    opacity = 0.8

    rects1 = plt.bar(index, bleu_train, bar_width,
                     alpha=opacity,
                     color='b',
                     label='train',
                     zorder=3)

    rects2 = plt.bar(index + bar_width, bleu_test, bar_width,
                     alpha=opacity,
                     color='r',
                     label='test',
                     zorder=3)

    plt.xlabel('BLEU')
    plt.ylabel('score')
    #plt.title('Scores')
    plt.xticks(index + bar_width, ('1', '2', '3', '4'))
    plt.legend()
    plt.tight_layout()
    plt.grid(True, axis='y')
    plt.savefig("BLEU_bar.png", dpi=200)
    plt.show()

testing_data=test_data
#test_dataset
print("Number of testing samples: ", len(testing_data))

all_gen_test=generate_caption_noimg(list(testing_data.keys()))
all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])
all_gen_test
for i in range(len(all_gen_test)):
  all_gen_test[i]='<start> '+ all_gen_test[i]+ ' <end>'
b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, all_gen_test)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

"""# **beam search**"""

#sampled_token_index11=[]
def generate_caption_noimg(data12,caption_model):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in data12:

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      start_token = "<start>"
      end_token = "<end>"
      #start = [wordtoix[start_token]]

      #start_word = [[start, 0.0]]


      for i in (range(max_decoded_sentence_length)):

          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token_index11.append(predictions[0, i, :])
          #print('sampled_token_index:',sampled_token_index)
          sampled_token = index_lookup[sampled_token_index]
          #print('sampled_token:',sampled_token)
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)
from math import log
from numpy import array
from numpy import argmax

# beam search
def beam_search_decoder(data, k):
	sequences = [[list(), 0.0]]
	# walk over each step in sequence
	for row in data:
		all_candidates = list()
		# expand each current candidate
		for i in range(len(sequences)):
			seq, score = sequences[i]
			for j in range(len(row)):
				candidate = [seq + [j], score - log(row[j])]
				all_candidates.append(candidate)


		# order all candidates by score
		ordered = sorted(all_candidates, key=lambda tup:tup[1])
		# select k best
		sequences = ordered[:k]
	return sequences

# define a sequence of 10 words over a vocab of 5 words

#greedy_result=(generate_caption_noimg([list(testing_data.keys())[3],caption_model))
#beam_search_result=beam_search_decoder(np.array(sampled_token_index11),5)

all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])

sampled_token_index11=[]
final_caption_using_beam_search_and_bleu_score=[]
list_test=list(testing_data.keys())

for k in tqdm(range (len(list_test))):
  greedy_result=(generate_caption_noimg([list_test[k]],caption_model))
  #print('greedy_result:',greedy_result)
  beam_search_result=beam_search_decoder(np.array(sampled_token_index11),10)
  #print(beam_search_result)


  caption_gen_test=[]
  for i in beam_search_result:
    text=''
    for j in i[0]:
      if j!=4:
        text=text+' '+index_lookup[j]
    #print('<start> '+text+ ' <end>')
    caption_gen_test.append('<start> '+text+ ' <end>')
  #print(caption_gen_test)
  b1=[]

  for caption in caption_gen_test:
    b1_test,b2_test,b3_test,b4_test = BLEU_corpus([all_true_test[k]], [caption])
    b1.append(b1_test)

  final_caption_using_beam_search_and_bleu_score.append(caption_gen_test[b1.index(max(b1))])
  #print(final_caption_using_beam_search_and_bleu_score)
    #print('##########################################################')

  sampled_token_index11=[]

for i in range(len(final_caption_using_beam_search_and_bleu_score)):
  final_caption_using_beam_search_and_bleu_score[i]=str(final_caption_using_beam_search_and_bleu_score[i]).replace('<end>','').strip()+' <end>'

import re
for i in range(len(final_caption_using_beam_search_and_bleu_score)):

  final_caption_using_beam_search_and_bleu_score[i]=re.sub(' +', ' ',final_caption_using_beam_search_and_bleu_score[i])

b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, final_caption_using_beam_search_and_bleu_score)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

data=pd.DataFrame(zip(all_true_test,all_gen_test,final_caption_using_beam_search_and_bleu_score),columns=['all_true_test','all_gen_test','final_caption_using_beam_search_with_k_10_and_bleu_score'])

data.to_excel('/save excels/ResNet101.xlsx')

"""scoers"""

!pip install pycocoevalcap
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice
import json


def bleu(gts,res):
    scorer = Bleu(n=4)
    # scorer += (hypo[0], ref1)   # hypo[0] = 'word1 word2 word3 ...'
    #                                 # ref = ['word1 word2 word3 ...', 'word1 word2 word3 ...']
    score, scores = scorer.compute_score(gts, res)
    print('belu = %s' % score)
    return score

def cider(gts,res):
    scorer = Cider()
    # scorer += (hypo[0], ref1)
    (score, scores) = scorer.compute_score(gts, res)
    print('cider = %s' % score)
    return score

def meteor(gts,res):
    scorer = Meteor()
    score, scores = scorer.compute_score(gts, res)
    print('meter = %s' % score)
    return score

def rouge(gts,res):
    scorer = Rouge()
    score, scores = scorer.compute_score(gts, res)
    print('rouge = %s' % score)
    return score

def spice(gts,res):
    scorer = Spice()
    score, scores = scorer.compute_score(gts, res)
    print('spice = %s' % score)
    return score

def main(gts,res):
    b1,b2,b3,b4=bleu(gts,res)
    cid=cider(gts,res)
    met=meteor(gts,res)
    rou=rouge(gts,res)
    spi=spice(gts,res)
    return b1,b2,b3,b4,cid,met,rou,spi

#score ith greedy search

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(all_gen_test[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)

#score with beam search k=10

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(final_caption_using_beam_search_and_bleu_score[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)



def get_cnn_model():
    base_model =  regnet.RegNetX120(
        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights="imagenet",
    )
    # We freeze our feature extractor
    base_model.trainable = False
    base_model_out = base_model.output
    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)
    cnn_model = keras.models.Model(base_model.input, base_model_out)
    return cnn_model

class TransformerEncoderBlock(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.0
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.dense_1 = layers.Dense(embed_dim, activation="relu")

    def call(self, inputs, training, mask=None):
        inputs = self.layernorm_1(inputs)
        inputs = self.dense_1(inputs)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=None,
            training=training,
        )
        out_1 = self.layernorm_2(inputs + attention_output_1)
        return out_1


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_tokens = embedded_tokens * self.embed_scale
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)


class TransformerDecoderBlock(layers.Layer):
    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.3
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.2
        )
        self.ffn_layer_1 = layers.Dense(ff_dim, activation="relu")
        self.ffn_layer_2 = layers.Dense(embed_dim)

        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()

        self.embedding = PositionalEmbedding(
            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE
        )
        self.out = layers.Dense(VOCAB_SIZE, activation="softmax")

        self.dropout_1 = layers.Dropout(0.3)#0.1
        self.dropout_2 = layers.Dropout(0.5)#0.1
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, training, mask=None):
        inputs = self.embedding(inputs)
        causal_mask = self.get_causal_attention_mask(inputs)

        if mask is not None:
            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)
            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
            combined_mask = tf.minimum(combined_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=combined_mask,
            training=training,
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
            training=training,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        ffn_out = self.ffn_layer_1(out_2)
        ffn_out = self.dropout_1(ffn_out, training=training)
        ffn_out = self.ffn_layer_2(ffn_out)

        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)
        ffn_out = self.dropout_2(ffn_out, training=training)
        preds = self.out(ffn_out)
        return preds

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)


class ImageCaptioningModel(keras.Model):
    def __init__(
        self, cnn_model, encoder, decoder, num_captions_per_image=3, image_aug=None,
    ):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.acc_tracker = keras.metrics.Mean(name="accuracy")
        self.num_captions_per_image = num_captions_per_image
        self.image_aug = image_aug

    def calculate_loss(self, y_true, y_pred, mask):
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)

    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        accuracy = tf.math.logical_and(mask, accuracy)
        accuracy = tf.cast(accuracy, dtype=tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)

    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):
        encoder_out = self.encoder(img_embed, training=training)
        batch_seq_inp = batch_seq[:, :-1]
        batch_seq_true = batch_seq[:, 1:]
        mask = tf.math.not_equal(batch_seq_true, 0)
        batch_seq_pred = self.decoder(
            batch_seq_inp, encoder_out, training=training, mask=mask
        )
        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)
        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)
        return loss, acc

    def train_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        if self.image_aug:
            batch_img = self.image_aug(batch_img)

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            with tf.GradientTape() as tape:
                loss, acc = self._compute_caption_loss_and_acc(
                    img_embed, batch_seq[:, i, :], training=True
                )

                # 3. Update loss and accuracy
                batch_loss += loss
                batch_acc += acc

            # 4. Get the list of all the trainable weights
            train_vars = (
                self.encoder.trainable_variables + self.decoder.trainable_variables
            )

            # 5. Get the gradients
            grads = tape.gradient(loss, train_vars)

            # 6. Update the trainable weights
            self.optimizer.apply_gradients(zip(grads, train_vars))

        # 7. Update the trackers
        batch_acc /= float(self.num_captions_per_image)
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 8. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    def test_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            loss, acc = self._compute_caption_loss_and_acc(
                img_embed, batch_seq[:, i, :], training=False
            )

            # 3. Update batch loss and batch accuracy
            batch_loss += loss
            batch_acc += acc

        batch_acc /= float(self.num_captions_per_image)

        # 4. Update the trackers
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 5. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    @property
    def metrics(self):
        # We need to list our metrics here so the `reset_states()` can be
        # called automatically.
        return [self.loss_tracker, self.acc_tracker]


cnn_model = get_cnn_model()
encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=5)
decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=6)
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,
)

# Define the loss function
cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction="none")

# EarlyStopping criteria
early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)


# Learning Rate Scheduler for the optimizer
class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, post_warmup_learning_rate, warmup_steps):
        super().__init__()
        self.post_warmup_learning_rate = post_warmup_learning_rate
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        global_step = tf.cast(step, tf.float32)
        warmup_steps = tf.cast(self.warmup_steps, tf.float32)
        warmup_progress = global_step / warmup_steps
        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress
        return tf.cond(
            global_step < warmup_steps,
            lambda: warmup_learning_rate,
            lambda: self.post_warmup_learning_rate,
        )


# Create a learning rate schedule
num_train_steps = len(train_dataset) * EPOCHS
num_warmup_steps = num_train_steps //15 #15
lr_schedule = LRSchedule(post_warmup_learning_rate= 1e-4, warmup_steps=num_warmup_steps)# 1e-4

# Compile the model
#caption_model.compile(optimizer=keras.optimizers.Adamax(lr_schedule), loss=cross_entropy)#loss=cross_entropy

#caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy
caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy

# Fit the model
caption_model.fit(train_dataset,epochs=EPOCHS,validation_data=valid_dataset,callbacks=[early_stopping])

# Compute definitive metrics on train/valid set
train_metrics = caption_model.evaluate(train_dataset, batch_size=BATCH_SIZE)
valid_metrics = caption_model.evaluate(valid_dataset, batch_size=BATCH_SIZE)

test_metrics = caption_model.evaluate(test_dataset, batch_size=BATCH_SIZE)

print("Train Loss = %.4f - Train Accuracy = %.4f" % (train_metrics[0], train_metrics[1]))
print("Valid Loss = %.4f - Valid Accuracy = %.4f" % (valid_metrics[0], valid_metrics[1]))

print("Test Loss = %.4f - Test Accuracy = %.4f" % (test_metrics[0], test_metrics[1]))

caption_model.save_weights('/saved model/RegNetX120/')
print('saved done!')
caption_model.load_weights('/saved model /RegNetX120/')



from tqdm import tqdm
"""
vocab = vectorization.get_vocabulary()
index_lookup = dict(zip(range(len(vocab)), vocab))
max_decoded_sentence_length = SEQ_LENGTH - 1
valid_images = list(valid_data.keys())
"""

def generate_caption_noimg(data12):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in tqdm(data12):

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      for i in range(max_decoded_sentence_length):
          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token = index_lookup[sampled_token_index]
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)

def chop_start_end(cpt):
    if cpt.startswith('<start>') and cpt.endswith('<end>'):
        cpt = ' '.join(cpt.split()[1:-1])
    return cpt

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def BLEU_corpus(actual_captions, generated_captions):
    list_of_references = []
    for images_captions in actual_captions:
        list_of_references.append([chop_start_end(cpt).split() for cpt in images_captions])
    hypotheses = [chop_start_end(cpt).split() for cpt in generated_captions]
    b1 = corpus_bleu(list_of_references, hypotheses, weights=(1, 0, 0, 0))
    b2 = corpus_bleu(list_of_references, hypotheses, weights=(0.5, 0.5, 0, 0))
    b3 = corpus_bleu(list_of_references, hypotheses, weights=(0.333, 0.333, 0.333, 0))
    b4 = corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))
    return b1, b2, b3, b4

def plot_bleu_bar_graph(bleu_train, bleu_test):
    n_groups = 4
    #bleu_train = (b1_train,b2_train,b3_train,b4_train)
    #bleu_test = (b1_test,b2_test,b3_test,b4_test)
    # create plot
    fig, ax = plt.subplots()
    index = np.arange(n_groups)
    bar_width = 0.3
    opacity = 0.8

    rects1 = plt.bar(index, bleu_train, bar_width,
                     alpha=opacity,
                     color='b',
                     label='train',
                     zorder=3)

    rects2 = plt.bar(index + bar_width, bleu_test, bar_width,
                     alpha=opacity,
                     color='r',
                     label='test',
                     zorder=3)

    plt.xlabel('BLEU')
    plt.ylabel('score')
    #plt.title('Scores')
    plt.xticks(index + bar_width, ('1', '2', '3', '4'))
    plt.legend()
    plt.tight_layout()
    plt.grid(True, axis='y')
    plt.savefig("BLEU_bar.png", dpi=200)
    plt.show()

testing_data=test_data
#test_dataset
print("Number of testing samples: ", len(testing_data))

all_gen_test=generate_caption_noimg(list(testing_data.keys()))
all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])
all_gen_test
for i in range(len(all_gen_test)):
  all_gen_test[i]='<start> '+ all_gen_test[i]+ ' <end>'
b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, all_gen_test)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

"""# **beam search**"""

#sampled_token_index11=[]
def generate_caption_noimg(data12,caption_model):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in data12:

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      start_token = "<start>"
      end_token = "<end>"
      #start = [wordtoix[start_token]]

      #start_word = [[start, 0.0]]


      for i in (range(max_decoded_sentence_length)):

          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token_index11.append(predictions[0, i, :])
          #print('sampled_token_index:',sampled_token_index)
          sampled_token = index_lookup[sampled_token_index]
          #print('sampled_token:',sampled_token)
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)
from math import log
from numpy import array
from numpy import argmax

# beam search
def beam_search_decoder(data, k):
	sequences = [[list(), 0.0]]
	# walk over each step in sequence
	for row in data:
		all_candidates = list()
		# expand each current candidate
		for i in range(len(sequences)):
			seq, score = sequences[i]
			for j in range(len(row)):
				candidate = [seq + [j], score - log(row[j])]
				all_candidates.append(candidate)


		# order all candidates by score
		ordered = sorted(all_candidates, key=lambda tup:tup[1])
		# select k best
		sequences = ordered[:k]
	return sequences

# define a sequence of 10 words over a vocab of 5 words

#greedy_result=(generate_caption_noimg([list(testing_data.keys())[3],caption_model))
#beam_search_result=beam_search_decoder(np.array(sampled_token_index11),5)

all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])

sampled_token_index11=[]
final_caption_using_beam_search_and_bleu_score=[]
list_test=list(testing_data.keys())

for k in tqdm(range (len(list_test))):
  greedy_result=(generate_caption_noimg([list_test[k]],caption_model))
  #print('greedy_result:',greedy_result)
  beam_search_result=beam_search_decoder(np.array(sampled_token_index11),10)
  #print(beam_search_result)


  caption_gen_test=[]
  for i in beam_search_result:
    text=''
    for j in i[0]:
      if j!=4:
        text=text+' '+index_lookup[j]
    #print('<start> '+text+ ' <end>')
    caption_gen_test.append('<start> '+text+ ' <end>')
  #print(caption_gen_test)
  b1=[]

  for caption in caption_gen_test:
    b1_test,b2_test,b3_test,b4_test = BLEU_corpus([all_true_test[k]], [caption])
    b1.append(b1_test)

  final_caption_using_beam_search_and_bleu_score.append(caption_gen_test[b1.index(max(b1))])
  #print(final_caption_using_beam_search_and_bleu_score)
    #print('##########################################################')

  sampled_token_index11=[]

for i in range(len(final_caption_using_beam_search_and_bleu_score)):
  final_caption_using_beam_search_and_bleu_score[i]=str(final_caption_using_beam_search_and_bleu_score[i]).replace('<end>','').strip()+' <end>'

import re
for i in range(len(final_caption_using_beam_search_and_bleu_score)):

  final_caption_using_beam_search_and_bleu_score[i]=re.sub(' +', ' ',final_caption_using_beam_search_and_bleu_score[i])

b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, final_caption_using_beam_search_and_bleu_score)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

data=pd.DataFrame(zip(all_true_test,all_gen_test,final_caption_using_beam_search_and_bleu_score),columns=['all_true_test','all_gen_test','final_caption_using_beam_search_with_k_10_and_bleu_score'])

data.to_excel('/save excels/RegNetX120.xlsx')

!pip install pycocoevalcap
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice
import json


def bleu(gts,res):
    scorer = Bleu(n=4)
    # scorer += (hypo[0], ref1)   # hypo[0] = 'word1 word2 word3 ...'
    #                                 # ref = ['word1 word2 word3 ...', 'word1 word2 word3 ...']
    score, scores = scorer.compute_score(gts, res)
    print('belu = %s' % score)
    return score

def cider(gts,res):
    scorer = Cider()
    # scorer += (hypo[0], ref1)
    (score, scores) = scorer.compute_score(gts, res)
    print('cider = %s' % score)
    return score

def meteor(gts,res):
    scorer = Meteor()
    score, scores = scorer.compute_score(gts, res)
    print('meter = %s' % score)
    return score

def rouge(gts,res):
    scorer = Rouge()
    score, scores = scorer.compute_score(gts, res)
    print('rouge = %s' % score)
    return score

def spice(gts,res):
    scorer = Spice()
    score, scores = scorer.compute_score(gts, res)
    print('spice = %s' % score)
    return score

def main(gts,res):
    b1,b2,b3,b4=bleu(gts,res)
    cid=cider(gts,res)
    met=meteor(gts,res)
    rou=rouge(gts,res)
    spi=spice(gts,res)
    return b1,b2,b3,b4,cid,met,rou,spi

#score ith greedy search

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(all_gen_test[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)

#score with beam search k=10

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(final_caption_using_beam_search_and_bleu_score[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)



def get_cnn_model():
    base_model =  vgg19.VGG19(
        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights="imagenet",
    )
    # We freeze our feature extractor
    base_model.trainable = False
    base_model_out = base_model.output
    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)
    cnn_model = keras.models.Model(base_model.input, base_model_out)
    return cnn_model

class TransformerEncoderBlock(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.0
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.dense_1 = layers.Dense(embed_dim, activation="relu")

    def call(self, inputs, training, mask=None):
        inputs = self.layernorm_1(inputs)
        inputs = self.dense_1(inputs)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=None,
            training=training,
        )
        out_1 = self.layernorm_2(inputs + attention_output_1)
        return out_1


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_tokens = embedded_tokens * self.embed_scale
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)


class TransformerDecoderBlock(layers.Layer):
    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.3
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.2
        )
        self.ffn_layer_1 = layers.Dense(ff_dim, activation="relu")
        self.ffn_layer_2 = layers.Dense(embed_dim)

        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()

        self.embedding = PositionalEmbedding(
            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE
        )
        self.out = layers.Dense(VOCAB_SIZE, activation="softmax")

        self.dropout_1 = layers.Dropout(0.3)#0.1
        self.dropout_2 = layers.Dropout(0.5)#0.1
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, training, mask=None):
        inputs = self.embedding(inputs)
        causal_mask = self.get_causal_attention_mask(inputs)

        if mask is not None:
            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)
            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
            combined_mask = tf.minimum(combined_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=combined_mask,
            training=training,
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
            training=training,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        ffn_out = self.ffn_layer_1(out_2)
        ffn_out = self.dropout_1(ffn_out, training=training)
        ffn_out = self.ffn_layer_2(ffn_out)

        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)
        ffn_out = self.dropout_2(ffn_out, training=training)
        preds = self.out(ffn_out)
        return preds

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)


class ImageCaptioningModel(keras.Model):
    def __init__(
        self, cnn_model, encoder, decoder, num_captions_per_image=3, image_aug=None,
    ):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.acc_tracker = keras.metrics.Mean(name="accuracy")
        self.num_captions_per_image = num_captions_per_image
        self.image_aug = image_aug

    def calculate_loss(self, y_true, y_pred, mask):
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)

    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        accuracy = tf.math.logical_and(mask, accuracy)
        accuracy = tf.cast(accuracy, dtype=tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)

    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):
        encoder_out = self.encoder(img_embed, training=training)
        batch_seq_inp = batch_seq[:, :-1]
        batch_seq_true = batch_seq[:, 1:]
        mask = tf.math.not_equal(batch_seq_true, 0)
        batch_seq_pred = self.decoder(
            batch_seq_inp, encoder_out, training=training, mask=mask
        )
        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)
        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)
        return loss, acc

    def train_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        if self.image_aug:
            batch_img = self.image_aug(batch_img)

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            with tf.GradientTape() as tape:
                loss, acc = self._compute_caption_loss_and_acc(
                    img_embed, batch_seq[:, i, :], training=True
                )

                # 3. Update loss and accuracy
                batch_loss += loss
                batch_acc += acc

            # 4. Get the list of all the trainable weights
            train_vars = (
                self.encoder.trainable_variables + self.decoder.trainable_variables
            )

            # 5. Get the gradients
            grads = tape.gradient(loss, train_vars)

            # 6. Update the trainable weights
            self.optimizer.apply_gradients(zip(grads, train_vars))

        # 7. Update the trackers
        batch_acc /= float(self.num_captions_per_image)
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 8. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    def test_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            loss, acc = self._compute_caption_loss_and_acc(
                img_embed, batch_seq[:, i, :], training=False
            )

            # 3. Update batch loss and batch accuracy
            batch_loss += loss
            batch_acc += acc

        batch_acc /= float(self.num_captions_per_image)

        # 4. Update the trackers
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 5. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    @property
    def metrics(self):
        # We need to list our metrics here so the `reset_states()` can be
        # called automatically.
        return [self.loss_tracker, self.acc_tracker]


cnn_model = get_cnn_model()
encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=5)
decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=6)
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,
)

"""# **Model training**"""

# Define the loss function
cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction="none")

# EarlyStopping criteria
early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)


# Learning Rate Scheduler for the optimizer
class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, post_warmup_learning_rate, warmup_steps):
        super().__init__()
        self.post_warmup_learning_rate = post_warmup_learning_rate
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        global_step = tf.cast(step, tf.float32)
        warmup_steps = tf.cast(self.warmup_steps, tf.float32)
        warmup_progress = global_step / warmup_steps
        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress
        return tf.cond(
            global_step < warmup_steps,
            lambda: warmup_learning_rate,
            lambda: self.post_warmup_learning_rate,
        )


# Create a learning rate schedule
num_train_steps = len(train_dataset) * EPOCHS
num_warmup_steps = num_train_steps //30 #15
lr_schedule = LRSchedule(post_warmup_learning_rate= 1e-4, warmup_steps=num_warmup_steps)# 1e-4


#caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy
caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy

# Fit the model
caption_model.fit(train_dataset,epochs=EPOCHS,validation_data=valid_dataset,callbacks=[early_stopping])

# Compute definitive metrics on train/valid set
train_metrics = caption_model.evaluate(train_dataset, batch_size=BATCH_SIZE)
valid_metrics = caption_model.evaluate(valid_dataset, batch_size=BATCH_SIZE)

test_metrics = caption_model.evaluate(test_dataset, batch_size=BATCH_SIZE)

print("Train Loss = %.4f - Train Accuracy = %.4f" % (train_metrics[0], train_metrics[1]))
print("Valid Loss = %.4f - Valid Accuracy = %.4f" % (valid_metrics[0], valid_metrics[1]))

print("Test Loss = %.4f - Test Accuracy = %.4f" % (test_metrics[0], test_metrics[1]))

"""**saved and load model**"""

caption_model.save_weights('/saved model/VGG19/')
print('saved done!')
caption_model.load_weights('/saved model/VGG19/')



"""# **prediction testing belue**"""

from tqdm import tqdm
"""
vocab = vectorization.get_vocabulary()
index_lookup = dict(zip(range(len(vocab)), vocab))
max_decoded_sentence_length = SEQ_LENGTH - 1
valid_images = list(valid_data.keys())
"""

def generate_caption_noimg(data12):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in tqdm(data12):

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      for i in range(max_decoded_sentence_length):
          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token = index_lookup[sampled_token_index]
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)

def chop_start_end(cpt):
    if cpt.startswith('<start>') and cpt.endswith('<end>'):
        cpt = ' '.join(cpt.split()[1:-1])
    return cpt

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def BLEU_corpus(actual_captions, generated_captions):
    list_of_references = []
    for images_captions in actual_captions:
        list_of_references.append([chop_start_end(cpt).split() for cpt in images_captions])
    hypotheses = [chop_start_end(cpt).split() for cpt in generated_captions]
    b1 = corpus_bleu(list_of_references, hypotheses, weights=(1, 0, 0, 0))
    b2 = corpus_bleu(list_of_references, hypotheses, weights=(0.5, 0.5, 0, 0))
    b3 = corpus_bleu(list_of_references, hypotheses, weights=(0.333, 0.333, 0.333, 0))
    b4 = corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))
    return b1, b2, b3, b4

def plot_bleu_bar_graph(bleu_train, bleu_test):
    n_groups = 4
    #bleu_train = (b1_train,b2_train,b3_train,b4_train)
    #bleu_test = (b1_test,b2_test,b3_test,b4_test)
    # create plot
    fig, ax = plt.subplots()
    index = np.arange(n_groups)
    bar_width = 0.3
    opacity = 0.8

    rects1 = plt.bar(index, bleu_train, bar_width,
                     alpha=opacity,
                     color='b',
                     label='train',
                     zorder=3)

    rects2 = plt.bar(index + bar_width, bleu_test, bar_width,
                     alpha=opacity,
                     color='r',
                     label='test',
                     zorder=3)

    plt.xlabel('BLEU')
    plt.ylabel('score')
    #plt.title('Scores')
    plt.xticks(index + bar_width, ('1', '2', '3', '4'))
    plt.legend()
    plt.tight_layout()
    plt.grid(True, axis='y')
    plt.savefig("BLEU_bar.png", dpi=200)
    plt.show()

testing_data=test_data
#test_dataset
print("Number of testing samples: ", len(testing_data))

all_gen_test=generate_caption_noimg(list(testing_data.keys()))
all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])
all_gen_test
for i in range(len(all_gen_test)):
  all_gen_test[i]='<start> '+ all_gen_test[i]+ ' <end>'
b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, all_gen_test)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

"""# **beam search**"""

#sampled_token_index11=[]
def generate_caption_noimg(data12,caption_model):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in data12:

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      start_token = "<start>"
      end_token = "<end>"
      #start = [wordtoix[start_token]]

      #start_word = [[start, 0.0]]


      for i in (range(max_decoded_sentence_length)):

          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token_index11.append(predictions[0, i, :])
          #print('sampled_token_index:',sampled_token_index)
          sampled_token = index_lookup[sampled_token_index]
          #print('sampled_token:',sampled_token)
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)
from math import log
from numpy import array
from numpy import argmax

# beam search
def beam_search_decoder(data, k):
	sequences = [[list(), 0.0]]
	# walk over each step in sequence
	for row in data:
		all_candidates = list()
		# expand each current candidate
		for i in range(len(sequences)):
			seq, score = sequences[i]
			for j in range(len(row)):
				candidate = [seq + [j], score - log(row[j])]
				all_candidates.append(candidate)


		# order all candidates by score
		ordered = sorted(all_candidates, key=lambda tup:tup[1])
		# select k best
		sequences = ordered[:k]
	return sequences

# define a sequence of 10 words over a vocab of 5 words

#greedy_result=(generate_caption_noimg([list(testing_data.keys())[3],caption_model))
#beam_search_result=beam_search_decoder(np.array(sampled_token_index11),5)

all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])

sampled_token_index11=[]
final_caption_using_beam_search_and_bleu_score=[]
list_test=list(testing_data.keys())

for k in tqdm(range (len(list_test))):
  greedy_result=(generate_caption_noimg([list_test[k]],caption_model))
  #print('greedy_result:',greedy_result)
  beam_search_result=beam_search_decoder(np.array(sampled_token_index11),10)
  #print(beam_search_result)


  caption_gen_test=[]
  for i in beam_search_result:
    text=''
    for j in i[0]:
      if j!=4:
        text=text+' '+index_lookup[j]
    #print('<start> '+text+ ' <end>')
    caption_gen_test.append('<start> '+text+ ' <end>')
  #print(caption_gen_test)
  b1=[]

  for caption in caption_gen_test:
    b1_test,b2_test,b3_test,b4_test = BLEU_corpus([all_true_test[k]], [caption])
    b1.append(b1_test)

  final_caption_using_beam_search_and_bleu_score.append(caption_gen_test[b1.index(max(b1))])
  #print(final_caption_using_beam_search_and_bleu_score)
    #print('##########################################################')

  sampled_token_index11=[]

for i in range(len(final_caption_using_beam_search_and_bleu_score)):
  final_caption_using_beam_search_and_bleu_score[i]=str(final_caption_using_beam_search_and_bleu_score[i]).replace('<end>','').strip()+' <end>'

import re
for i in range(len(final_caption_using_beam_search_and_bleu_score)):

  final_caption_using_beam_search_and_bleu_score[i]=re.sub(' +', ' ',final_caption_using_beam_search_and_bleu_score[i])

b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, final_caption_using_beam_search_and_bleu_score)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

data=pd.DataFrame(zip(all_true_test,all_gen_test,final_caption_using_beam_search_and_bleu_score),columns=['all_true_test','all_gen_test','final_caption_using_beam_search_with_k_10_and_bleu_score'])

data.to_excel('/save excels/VGG19.xlsx')

"""scoers"""

!pip install pycocoevalcap
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice
import json


def bleu(gts,res):
    scorer = Bleu(n=4)
    # scorer += (hypo[0], ref1)   # hypo[0] = 'word1 word2 word3 ...'
    #                                 # ref = ['word1 word2 word3 ...', 'word1 word2 word3 ...']
    score, scores = scorer.compute_score(gts, res)
    print('belu = %s' % score)
    return score

def cider(gts,res):
    scorer = Cider()
    # scorer += (hypo[0], ref1)
    (score, scores) = scorer.compute_score(gts, res)
    print('cider = %s' % score)
    return score

def meteor(gts,res):
    scorer = Meteor()
    score, scores = scorer.compute_score(gts, res)
    print('meter = %s' % score)
    return score

def rouge(gts,res):
    scorer = Rouge()
    score, scores = scorer.compute_score(gts, res)
    print('rouge = %s' % score)
    return score

def spice(gts,res):
    scorer = Spice()
    score, scores = scorer.compute_score(gts, res)
    print('spice = %s' % score)
    return score

def main(gts,res):
    b1,b2,b3,b4=bleu(gts,res)
    cid=cider(gts,res)
    met=meteor(gts,res)
    rou=rouge(gts,res)
    spi=spice(gts,res)
    return b1,b2,b3,b4,cid,met,rou,spi

#score ith greedy search

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(all_gen_test[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)

#score with beam search k=10

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(final_caption_using_beam_search_and_bleu_score[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)















def get_cnn_model():
    base_model =   vgg16.VGG16(
        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights="imagenet",
    )
    # We freeze our feature extractor
    base_model.trainable = False
    base_model_out = base_model.output
    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)
    cnn_model = keras.models.Model(base_model.input, base_model_out)
    return cnn_model

class TransformerEncoderBlock(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.0
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.dense_1 = layers.Dense(embed_dim, activation="relu")

    def call(self, inputs, training, mask=None):
        inputs = self.layernorm_1(inputs)
        inputs = self.dense_1(inputs)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=None,
            training=training,
        )
        out_1 = self.layernorm_2(inputs + attention_output_1)
        return out_1


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_tokens = embedded_tokens * self.embed_scale
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)


class TransformerDecoderBlock(layers.Layer):
    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.3
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.2
        )
        self.ffn_layer_1 = layers.Dense(ff_dim, activation="relu")
        self.ffn_layer_2 = layers.Dense(embed_dim)

        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()

        self.embedding = PositionalEmbedding(
            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE
        )
        self.out = layers.Dense(VOCAB_SIZE, activation="softmax")

        self.dropout_1 = layers.Dropout(0.3)#0.1
        self.dropout_2 = layers.Dropout(0.5)#0.1
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, training, mask=None):
        inputs = self.embedding(inputs)
        causal_mask = self.get_causal_attention_mask(inputs)

        if mask is not None:
            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)
            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
            combined_mask = tf.minimum(combined_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=combined_mask,
            training=training,
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
            training=training,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        ffn_out = self.ffn_layer_1(out_2)
        ffn_out = self.dropout_1(ffn_out, training=training)
        ffn_out = self.ffn_layer_2(ffn_out)

        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)
        ffn_out = self.dropout_2(ffn_out, training=training)
        preds = self.out(ffn_out)
        return preds

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)


class ImageCaptioningModel(keras.Model):
    def __init__(
        self, cnn_model, encoder, decoder, num_captions_per_image=3, image_aug=None,
    ):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.acc_tracker = keras.metrics.Mean(name="accuracy")
        self.num_captions_per_image = num_captions_per_image
        self.image_aug = image_aug

    def calculate_loss(self, y_true, y_pred, mask):
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)

    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        accuracy = tf.math.logical_and(mask, accuracy)
        accuracy = tf.cast(accuracy, dtype=tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)

    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):
        encoder_out = self.encoder(img_embed, training=training)
        batch_seq_inp = batch_seq[:, :-1]
        batch_seq_true = batch_seq[:, 1:]
        mask = tf.math.not_equal(batch_seq_true, 0)
        batch_seq_pred = self.decoder(
            batch_seq_inp, encoder_out, training=training, mask=mask
        )
        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)
        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)
        return loss, acc

    def train_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        if self.image_aug:
            batch_img = self.image_aug(batch_img)

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            with tf.GradientTape() as tape:
                loss, acc = self._compute_caption_loss_and_acc(
                    img_embed, batch_seq[:, i, :], training=True
                )

                # 3. Update loss and accuracy
                batch_loss += loss
                batch_acc += acc

            # 4. Get the list of all the trainable weights
            train_vars = (
                self.encoder.trainable_variables + self.decoder.trainable_variables
            )

            # 5. Get the gradients
            grads = tape.gradient(loss, train_vars)

            # 6. Update the trainable weights
            self.optimizer.apply_gradients(zip(grads, train_vars))

        # 7. Update the trackers
        batch_acc /= float(self.num_captions_per_image)
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 8. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    def test_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            loss, acc = self._compute_caption_loss_and_acc(
                img_embed, batch_seq[:, i, :], training=False
            )

            # 3. Update batch loss and batch accuracy
            batch_loss += loss
            batch_acc += acc

        batch_acc /= float(self.num_captions_per_image)

        # 4. Update the trackers
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 5. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    @property
    def metrics(self):
        # We need to list our metrics here so the `reset_states()` can be
        # called automatically.
        return [self.loss_tracker, self.acc_tracker]


cnn_model = get_cnn_model()
encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=5)
decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=6)
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,
)



# Define the loss function
cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction="none")

# EarlyStopping criteria
early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)


# Learning Rate Scheduler for the optimizer
class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, post_warmup_learning_rate, warmup_steps):
        super().__init__()
        self.post_warmup_learning_rate = post_warmup_learning_rate
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        global_step = tf.cast(step, tf.float32)
        warmup_steps = tf.cast(self.warmup_steps, tf.float32)
        warmup_progress = global_step / warmup_steps
        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress
        return tf.cond(
            global_step < warmup_steps,
            lambda: warmup_learning_rate,
            lambda: self.post_warmup_learning_rate,
        )


# Create a learning rate schedule
num_train_steps = len(train_dataset) * EPOCHS
num_warmup_steps = num_train_steps //15 #15
lr_schedule = LRSchedule(post_warmup_learning_rate= 1e-4, warmup_steps=num_warmup_steps)# 1e-4

# Compile the model
#caption_model.compile(optimizer=keras.optimizers.Adamax(lr_schedule), loss=cross_entropy)#loss=cross_entropy

#caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy
caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy

# Fit the model
caption_model.fit(train_dataset,epochs=EPOCHS,validation_data=valid_dataset,callbacks=[early_stopping])



# Compute definitive metrics on train/valid set
train_metrics = caption_model.evaluate(train_dataset, batch_size=BATCH_SIZE)
valid_metrics = caption_model.evaluate(valid_dataset, batch_size=BATCH_SIZE)

test_metrics = caption_model.evaluate(test_dataset, batch_size=BATCH_SIZE)

print("Train Loss = %.4f - Train Accuracy = %.4f" % (train_metrics[0], train_metrics[1]))
print("Valid Loss = %.4f - Valid Accuracy = %.4f" % (valid_metrics[0], valid_metrics[1]))

print("Test Loss = %.4f - Test Accuracy = %.4f" % (test_metrics[0], test_metrics[1]))

caption_model.save_weights('/saved model/ VGG16/')
print('saved done!')
caption_model.load_weights('/saved model / VGG16/')

"""# **prediction testing belue**"""

from tqdm import tqdm
"""
vocab = vectorization.get_vocabulary()
index_lookup = dict(zip(range(len(vocab)), vocab))
max_decoded_sentence_length = SEQ_LENGTH - 1
valid_images = list(valid_data.keys())
"""

def generate_caption_noimg(data12):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in tqdm(data12):

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      for i in range(max_decoded_sentence_length):
          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token = index_lookup[sampled_token_index]
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)

def chop_start_end(cpt):
    if cpt.startswith('<start>') and cpt.endswith('<end>'):
        cpt = ' '.join(cpt.split()[1:-1])
    return cpt

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def BLEU_corpus(actual_captions, generated_captions):
    list_of_references = []
    for images_captions in actual_captions:
        list_of_references.append([chop_start_end(cpt).split() for cpt in images_captions])
    hypotheses = [chop_start_end(cpt).split() for cpt in generated_captions]
    b1 = corpus_bleu(list_of_references, hypotheses, weights=(1, 0, 0, 0))
    b2 = corpus_bleu(list_of_references, hypotheses, weights=(0.5, 0.5, 0, 0))
    b3 = corpus_bleu(list_of_references, hypotheses, weights=(0.333, 0.333, 0.333, 0))
    b4 = corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))
    return b1, b2, b3, b4

def plot_bleu_bar_graph(bleu_train, bleu_test):
    n_groups = 4
    #bleu_train = (b1_train,b2_train,b3_train,b4_train)
    #bleu_test = (b1_test,b2_test,b3_test,b4_test)
    # create plot
    fig, ax = plt.subplots()
    index = np.arange(n_groups)
    bar_width = 0.3
    opacity = 0.8

    rects1 = plt.bar(index, bleu_train, bar_width,
                     alpha=opacity,
                     color='b',
                     label='train',
                     zorder=3)

    rects2 = plt.bar(index + bar_width, bleu_test, bar_width,
                     alpha=opacity,
                     color='r',
                     label='test',
                     zorder=3)

    plt.xlabel('BLEU')
    plt.ylabel('score')
    #plt.title('Scores')
    plt.xticks(index + bar_width, ('1', '2', '3', '4'))
    plt.legend()
    plt.tight_layout()
    plt.grid(True, axis='y')
    plt.savefig("BLEU_bar.png", dpi=200)
    plt.show()

testing_data=test_data
#test_dataset
print("Number of testing samples: ", len(testing_data))

all_gen_test=generate_caption_noimg(list(testing_data.keys()))
all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])
all_gen_test
for i in range(len(all_gen_test)):
  all_gen_test[i]='<start> '+ all_gen_test[i]+ ' <end>'
b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, all_gen_test)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

"""# **beam search**"""

#sampled_token_index11=[]
def generate_caption_noimg(data12,caption_model):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in data12:

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      start_token = "<start>"
      end_token = "<end>"
      #start = [wordtoix[start_token]]

      #start_word = [[start, 0.0]]


      for i in (range(max_decoded_sentence_length)):

          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token_index11.append(predictions[0, i, :])
          #print('sampled_token_index:',sampled_token_index)
          sampled_token = index_lookup[sampled_token_index]
          #print('sampled_token:',sampled_token)
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)
from math import log
from numpy import array
from numpy import argmax

# beam search
def beam_search_decoder(data, k):
	sequences = [[list(), 0.0]]
	# walk over each step in sequence
	for row in data:
		all_candidates = list()
		# expand each current candidate
		for i in range(len(sequences)):
			seq, score = sequences[i]
			for j in range(len(row)):
				candidate = [seq + [j], score - log(row[j])]
				all_candidates.append(candidate)


		# order all candidates by score
		ordered = sorted(all_candidates, key=lambda tup:tup[1])
		# select k best
		sequences = ordered[:k]
	return sequences

# define a sequence of 10 words over a vocab of 5 words

#greedy_result=(generate_caption_noimg([list(testing_data.keys())[3],caption_model))
#beam_search_result=beam_search_decoder(np.array(sampled_token_index11),5)

all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])

sampled_token_index11=[]
final_caption_using_beam_search_and_bleu_score=[]
list_test=list(testing_data.keys())

for k in tqdm(range (len(list_test))):
  greedy_result=(generate_caption_noimg([list_test[k]],caption_model))
  #print('greedy_result:',greedy_result)
  beam_search_result=beam_search_decoder(np.array(sampled_token_index11),10)
  #print(beam_search_result)


  caption_gen_test=[]
  for i in beam_search_result:
    text=''
    for j in i[0]:
      if j!=4:
        text=text+' '+index_lookup[j]
    #print('<start> '+text+ ' <end>')
    caption_gen_test.append('<start> '+text+ ' <end>')
  #print(caption_gen_test)
  b1=[]

  for caption in caption_gen_test:
    b1_test,b2_test,b3_test,b4_test = BLEU_corpus([all_true_test[k]], [caption])
    b1.append(b1_test)

  final_caption_using_beam_search_and_bleu_score.append(caption_gen_test[b1.index(max(b1))])
  #print(final_caption_using_beam_search_and_bleu_score)
    #print('##########################################################')

  sampled_token_index11=[]

for i in range(len(final_caption_using_beam_search_and_bleu_score)):
  final_caption_using_beam_search_and_bleu_score[i]=str(final_caption_using_beam_search_and_bleu_score[i]).replace('<end>','').strip()+' <end>'

import re
for i in range(len(final_caption_using_beam_search_and_bleu_score)):

  final_caption_using_beam_search_and_bleu_score[i]=re.sub(' +', ' ',final_caption_using_beam_search_and_bleu_score[i])

b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, final_caption_using_beam_search_and_bleu_score)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

data=pd.DataFrame(zip(all_true_test,all_gen_test,final_caption_using_beam_search_and_bleu_score),columns=['all_true_test','all_gen_test','final_caption_using_beam_search_with_k_10_and_bleu_score'])

data.to_excel('/save excels/VGG16.xlsx')

"""scoers"""

!pip install pycocoevalcap
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice
import json


def bleu(gts,res):
    scorer = Bleu(n=4)
    # scorer += (hypo[0], ref1)   # hypo[0] = 'word1 word2 word3 ...'
    #                                 # ref = ['word1 word2 word3 ...', 'word1 word2 word3 ...']
    score, scores = scorer.compute_score(gts, res)
    print('belu = %s' % score)
    return score

def cider(gts,res):
    scorer = Cider()
    # scorer += (hypo[0], ref1)
    (score, scores) = scorer.compute_score(gts, res)
    print('cider = %s' % score)
    return score

def meteor(gts,res):
    scorer = Meteor()
    score, scores = scorer.compute_score(gts, res)
    print('meter = %s' % score)
    return score

def rouge(gts,res):
    scorer = Rouge()
    score, scores = scorer.compute_score(gts, res)
    print('rouge = %s' % score)
    return score

def spice(gts,res):
    scorer = Spice()
    score, scores = scorer.compute_score(gts, res)
    print('spice = %s' % score)
    return score

def main(gts,res):
    b1,b2,b3,b4=bleu(gts,res)
    cid=cider(gts,res)
    met=meteor(gts,res)
    rou=rouge(gts,res)
    spi=spice(gts,res)
    return b1,b2,b3,b4,cid,met,rou,spi

#score ith greedy search

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(all_gen_test[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)

#score with beam search k=10

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(final_caption_using_beam_search_and_bleu_score[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)



def get_cnn_model():
    base_model =  efficientnet.EfficientNetB2(
        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights="imagenet",
    )
    # We freeze our feature extractor
    base_model.trainable = False
    base_model_out = base_model.output
    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)
    cnn_model = keras.models.Model(base_model.input, base_model_out)
    return cnn_model

class TransformerEncoderBlock(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.0
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.dense_1 = layers.Dense(embed_dim, activation="relu")

    def call(self, inputs, training, mask=None):
        inputs = self.layernorm_1(inputs)
        inputs = self.dense_1(inputs)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=None,
            training=training,
        )
        out_1 = self.layernorm_2(inputs + attention_output_1)
        return out_1


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_tokens = embedded_tokens * self.embed_scale
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)


class TransformerDecoderBlock(layers.Layer):
    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.3
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.2
        )
        self.ffn_layer_1 = layers.Dense(ff_dim, activation="relu")
        self.ffn_layer_2 = layers.Dense(embed_dim)

        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()

        self.embedding = PositionalEmbedding(
            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE
        )
        self.out = layers.Dense(VOCAB_SIZE, activation="softmax")

        self.dropout_1 = layers.Dropout(0.3)#0.1
        self.dropout_2 = layers.Dropout(0.5)#0.1
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, training, mask=None):
        inputs = self.embedding(inputs)
        causal_mask = self.get_causal_attention_mask(inputs)

        if mask is not None:
            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)
            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
            combined_mask = tf.minimum(combined_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=combined_mask,
            training=training,
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
            training=training,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        ffn_out = self.ffn_layer_1(out_2)
        ffn_out = self.dropout_1(ffn_out, training=training)
        ffn_out = self.ffn_layer_2(ffn_out)

        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)
        ffn_out = self.dropout_2(ffn_out, training=training)
        preds = self.out(ffn_out)
        return preds

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)


class ImageCaptioningModel(keras.Model):
    def __init__(
        self, cnn_model, encoder, decoder, num_captions_per_image=3, image_aug=None,
    ):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.acc_tracker = keras.metrics.Mean(name="accuracy")
        self.num_captions_per_image = num_captions_per_image
        self.image_aug = image_aug

    def calculate_loss(self, y_true, y_pred, mask):
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)

    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        accuracy = tf.math.logical_and(mask, accuracy)
        accuracy = tf.cast(accuracy, dtype=tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)

    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):
        encoder_out = self.encoder(img_embed, training=training)
        batch_seq_inp = batch_seq[:, :-1]
        batch_seq_true = batch_seq[:, 1:]
        mask = tf.math.not_equal(batch_seq_true, 0)
        batch_seq_pred = self.decoder(
            batch_seq_inp, encoder_out, training=training, mask=mask
        )
        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)
        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)
        return loss, acc

    def train_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        if self.image_aug:
            batch_img = self.image_aug(batch_img)

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            with tf.GradientTape() as tape:
                loss, acc = self._compute_caption_loss_and_acc(
                    img_embed, batch_seq[:, i, :], training=True
                )

                # 3. Update loss and accuracy
                batch_loss += loss
                batch_acc += acc

            # 4. Get the list of all the trainable weights
            train_vars = (
                self.encoder.trainable_variables + self.decoder.trainable_variables
            )

            # 5. Get the gradients
            grads = tape.gradient(loss, train_vars)

            # 6. Update the trainable weights
            self.optimizer.apply_gradients(zip(grads, train_vars))

        # 7. Update the trackers
        batch_acc /= float(self.num_captions_per_image)
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 8. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    def test_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            loss, acc = self._compute_caption_loss_and_acc(
                img_embed, batch_seq[:, i, :], training=False
            )

            # 3. Update batch loss and batch accuracy
            batch_loss += loss
            batch_acc += acc

        batch_acc /= float(self.num_captions_per_image)

        # 4. Update the trackers
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 5. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    @property
    def metrics(self):
        # We need to list our metrics here so the `reset_states()` can be
        # called automatically.
        return [self.loss_tracker, self.acc_tracker]


cnn_model = get_cnn_model()
encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=5)
decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=6)
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,
)

# Define the loss function
cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction="none")

# EarlyStopping criteria
early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)


# Learning Rate Scheduler for the optimizer
class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, post_warmup_learning_rate, warmup_steps):
        super().__init__()
        self.post_warmup_learning_rate = post_warmup_learning_rate
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        global_step = tf.cast(step, tf.float32)
        warmup_steps = tf.cast(self.warmup_steps, tf.float32)
        warmup_progress = global_step / warmup_steps
        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress
        return tf.cond(
            global_step < warmup_steps,
            lambda: warmup_learning_rate,
            lambda: self.post_warmup_learning_rate,
        )


# Create a learning rate schedule
num_train_steps = len(train_dataset) * EPOCHS
num_warmup_steps = num_train_steps //15 #15
lr_schedule = LRSchedule(post_warmup_learning_rate= 1e-4, warmup_steps=num_warmup_steps)# 1e-4

# Compile the model
#caption_model.compile(optimizer=keras.optimizers.Adamax(lr_schedule), loss=cross_entropy)#loss=cross_entropy

#caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy
caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy

# Fit the model
caption_model.fit(train_dataset,epochs=EPOCHS,validation_data=valid_dataset,callbacks=[early_stopping])

# Compute definitive metrics on train/valid set
train_metrics = caption_model.evaluate(train_dataset, batch_size=BATCH_SIZE)
valid_metrics = caption_model.evaluate(valid_dataset, batch_size=BATCH_SIZE)

test_metrics = caption_model.evaluate(test_dataset, batch_size=BATCH_SIZE)

print("Train Loss = %.4f - Train Accuracy = %.4f" % (train_metrics[0], train_metrics[1]))
print("Valid Loss = %.4f - Valid Accuracy = %.4f" % (valid_metrics[0], valid_metrics[1]))

print("Test Loss = %.4f - Test Accuracy = %.4f" % (test_metrics[0], test_metrics[1]))

caption_model.save_weights('/saved model/EfficientNetB2/')
print('saved done!')
caption_model.load_weights('/saved model /EfficientNetB2/')



from tqdm import tqdm
"""
vocab = vectorization.get_vocabulary()
index_lookup = dict(zip(range(len(vocab)), vocab))
max_decoded_sentence_length = SEQ_LENGTH - 1
valid_images = list(valid_data.keys())
"""

def generate_caption_noimg(data12):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in tqdm(data12):

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      for i in range(max_decoded_sentence_length):
          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token = index_lookup[sampled_token_index]
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)

def chop_start_end(cpt):
    if cpt.startswith('<start>') and cpt.endswith('<end>'):
        cpt = ' '.join(cpt.split()[1:-1])
    return cpt

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def BLEU_corpus(actual_captions, generated_captions):
    list_of_references = []
    for images_captions in actual_captions:
        list_of_references.append([chop_start_end(cpt).split() for cpt in images_captions])
    hypotheses = [chop_start_end(cpt).split() for cpt in generated_captions]
    b1 = corpus_bleu(list_of_references, hypotheses, weights=(1, 0, 0, 0))
    b2 = corpus_bleu(list_of_references, hypotheses, weights=(0.5, 0.5, 0, 0))
    b3 = corpus_bleu(list_of_references, hypotheses, weights=(0.333, 0.333, 0.333, 0))
    b4 = corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))
    return b1, b2, b3, b4

def plot_bleu_bar_graph(bleu_train, bleu_test):
    n_groups = 4
    #bleu_train = (b1_train,b2_train,b3_train,b4_train)
    #bleu_test = (b1_test,b2_test,b3_test,b4_test)
    # create plot
    fig, ax = plt.subplots()
    index = np.arange(n_groups)
    bar_width = 0.3
    opacity = 0.8

    rects1 = plt.bar(index, bleu_train, bar_width,
                     alpha=opacity,
                     color='b',
                     label='train',
                     zorder=3)

    rects2 = plt.bar(index + bar_width, bleu_test, bar_width,
                     alpha=opacity,
                     color='r',
                     label='test',
                     zorder=3)

    plt.xlabel('BLEU')
    plt.ylabel('score')
    #plt.title('Scores')
    plt.xticks(index + bar_width, ('1', '2', '3', '4'))
    plt.legend()
    plt.tight_layout()
    plt.grid(True, axis='y')
    plt.savefig("BLEU_bar.png", dpi=200)
    plt.show()

testing_data=test_data
#test_dataset
print("Number of testing samples: ", len(testing_data))

all_gen_test=generate_caption_noimg(list(testing_data.keys()))
all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])
all_gen_test
for i in range(len(all_gen_test)):
  all_gen_test[i]='<start> '+ all_gen_test[i]+ ' <end>'
b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, all_gen_test)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

"""# **beam search**"""

#sampled_token_index11=[]
def generate_caption_noimg(data12,caption_model):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in data12:

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      start_token = "<start>"
      end_token = "<end>"
      #start = [wordtoix[start_token]]

      #start_word = [[start, 0.0]]


      for i in (range(max_decoded_sentence_length)):

          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token_index11.append(predictions[0, i, :])
          #print('sampled_token_index:',sampled_token_index)
          sampled_token = index_lookup[sampled_token_index]
          #print('sampled_token:',sampled_token)
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)
from math import log
from numpy import array
from numpy import argmax

# beam search
def beam_search_decoder(data, k):
	sequences = [[list(), 0.0]]
	# walk over each step in sequence
	for row in data:
		all_candidates = list()
		# expand each current candidate
		for i in range(len(sequences)):
			seq, score = sequences[i]
			for j in range(len(row)):
				candidate = [seq + [j], score - log(row[j])]
				all_candidates.append(candidate)


		# order all candidates by score
		ordered = sorted(all_candidates, key=lambda tup:tup[1])
		# select k best
		sequences = ordered[:k]
	return sequences

# define a sequence of 10 words over a vocab of 5 words

#greedy_result=(generate_caption_noimg([list(testing_data.keys())[3],caption_model))
#beam_search_result=beam_search_decoder(np.array(sampled_token_index11),5)

all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])

sampled_token_index11=[]
final_caption_using_beam_search_and_bleu_score=[]
list_test=list(testing_data.keys())

for k in tqdm(range (len(list_test))):
  greedy_result=(generate_caption_noimg([list_test[k]],caption_model))
  #print('greedy_result:',greedy_result)
  beam_search_result=beam_search_decoder(np.array(sampled_token_index11),10)
  #print(beam_search_result)


  caption_gen_test=[]
  for i in beam_search_result:
    text=''
    for j in i[0]:
      if j!=4:
        text=text+' '+index_lookup[j]
    #print('<start> '+text+ ' <end>')
    caption_gen_test.append('<start> '+text+ ' <end>')
  #print(caption_gen_test)
  b1=[]

  for caption in caption_gen_test:
    b1_test,b2_test,b3_test,b4_test = BLEU_corpus([all_true_test[k]], [caption])
    b1.append(b1_test)

  final_caption_using_beam_search_and_bleu_score.append(caption_gen_test[b1.index(max(b1))])
  #print(final_caption_using_beam_search_and_bleu_score)
    #print('##########################################################')

  sampled_token_index11=[]

for i in range(len(final_caption_using_beam_search_and_bleu_score)):
  final_caption_using_beam_search_and_bleu_score[i]=str(final_caption_using_beam_search_and_bleu_score[i]).replace('<end>','').strip()+' <end>'

import re
for i in range(len(final_caption_using_beam_search_and_bleu_score)):

  final_caption_using_beam_search_and_bleu_score[i]=re.sub(' +', ' ',final_caption_using_beam_search_and_bleu_score[i])

b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, final_caption_using_beam_search_and_bleu_score)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

data=pd.DataFrame(zip(all_true_test,all_gen_test,final_caption_using_beam_search_and_bleu_score),columns=['all_true_test','all_gen_test','final_caption_using_beam_search_with_k_10_and_bleu_score'])

data.to_excel('/save excels/EfficientNetB2.xlsx')

!pip install pycocoevalcap
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice
import json


def bleu(gts,res):
    scorer = Bleu(n=4)
    # scorer += (hypo[0], ref1)   # hypo[0] = 'word1 word2 word3 ...'
    #                                 # ref = ['word1 word2 word3 ...', 'word1 word2 word3 ...']
    score, scores = scorer.compute_score(gts, res)
    print('belu = %s' % score)
    return score

def cider(gts,res):
    scorer = Cider()
    # scorer += (hypo[0], ref1)
    (score, scores) = scorer.compute_score(gts, res)
    print('cider = %s' % score)
    return score

def meteor(gts,res):
    scorer = Meteor()
    score, scores = scorer.compute_score(gts, res)
    print('meter = %s' % score)
    return score

def rouge(gts,res):
    scorer = Rouge()
    score, scores = scorer.compute_score(gts, res)
    print('rouge = %s' % score)
    return score

def spice(gts,res):
    scorer = Spice()
    score, scores = scorer.compute_score(gts, res)
    print('spice = %s' % score)
    return score

def main(gts,res):
    b1,b2,b3,b4=bleu(gts,res)
    cid=cider(gts,res)
    met=meteor(gts,res)
    rou=rouge(gts,res)
    spi=spice(gts,res)
    return b1,b2,b3,b4,cid,met,rou,spi

#score ith greedy search

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(all_gen_test[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)

#score with beam search k=10

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(final_caption_using_beam_search_and_bleu_score[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)



def get_cnn_model():
    base_model =  efficientnet_v2.EfficientNetV2B0(
        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights="imagenet",
    )
    # We freeze our feature extractor
    base_model.trainable = False
    base_model_out = base_model.output
    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)
    cnn_model = keras.models.Model(base_model.input, base_model_out)
    return cnn_model

class TransformerEncoderBlock(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.0
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.dense_1 = layers.Dense(embed_dim, activation="relu")

    def call(self, inputs, training, mask=None):
        inputs = self.layernorm_1(inputs)
        inputs = self.dense_1(inputs)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=None,
            training=training,
        )
        out_1 = self.layernorm_2(inputs + attention_output_1)
        return out_1


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_tokens = embedded_tokens * self.embed_scale
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)


class TransformerDecoderBlock(layers.Layer):
    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.3
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.2
        )
        self.ffn_layer_1 = layers.Dense(ff_dim, activation="relu")
        self.ffn_layer_2 = layers.Dense(embed_dim)

        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()

        self.embedding = PositionalEmbedding(
            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE
        )
        self.out = layers.Dense(VOCAB_SIZE, activation="softmax")

        self.dropout_1 = layers.Dropout(0.3)#0.1
        self.dropout_2 = layers.Dropout(0.5)#0.1
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, training, mask=None):
        inputs = self.embedding(inputs)
        causal_mask = self.get_causal_attention_mask(inputs)

        if mask is not None:
            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)
            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
            combined_mask = tf.minimum(combined_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=combined_mask,
            training=training,
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
            training=training,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        ffn_out = self.ffn_layer_1(out_2)
        ffn_out = self.dropout_1(ffn_out, training=training)
        ffn_out = self.ffn_layer_2(ffn_out)

        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)
        ffn_out = self.dropout_2(ffn_out, training=training)
        preds = self.out(ffn_out)
        return preds

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)


class ImageCaptioningModel(keras.Model):
    def __init__(
        self, cnn_model, encoder, decoder, num_captions_per_image=3, image_aug=None,
    ):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.acc_tracker = keras.metrics.Mean(name="accuracy")
        self.num_captions_per_image = num_captions_per_image
        self.image_aug = image_aug

    def calculate_loss(self, y_true, y_pred, mask):
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)

    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        accuracy = tf.math.logical_and(mask, accuracy)
        accuracy = tf.cast(accuracy, dtype=tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)

    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):
        encoder_out = self.encoder(img_embed, training=training)
        batch_seq_inp = batch_seq[:, :-1]
        batch_seq_true = batch_seq[:, 1:]
        mask = tf.math.not_equal(batch_seq_true, 0)
        batch_seq_pred = self.decoder(
            batch_seq_inp, encoder_out, training=training, mask=mask
        )
        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)
        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)
        return loss, acc

    def train_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        if self.image_aug:
            batch_img = self.image_aug(batch_img)

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            with tf.GradientTape() as tape:
                loss, acc = self._compute_caption_loss_and_acc(
                    img_embed, batch_seq[:, i, :], training=True
                )

                # 3. Update loss and accuracy
                batch_loss += loss
                batch_acc += acc

            # 4. Get the list of all the trainable weights
            train_vars = (
                self.encoder.trainable_variables + self.decoder.trainable_variables
            )

            # 5. Get the gradients
            grads = tape.gradient(loss, train_vars)

            # 6. Update the trainable weights
            self.optimizer.apply_gradients(zip(grads, train_vars))

        # 7. Update the trackers
        batch_acc /= float(self.num_captions_per_image)
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 8. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    def test_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            loss, acc = self._compute_caption_loss_and_acc(
                img_embed, batch_seq[:, i, :], training=False
            )

            # 3. Update batch loss and batch accuracy
            batch_loss += loss
            batch_acc += acc

        batch_acc /= float(self.num_captions_per_image)

        # 4. Update the trackers
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 5. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    @property
    def metrics(self):
        # We need to list our metrics here so the `reset_states()` can be
        # called automatically.
        return [self.loss_tracker, self.acc_tracker]


cnn_model = get_cnn_model()
encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=5)
decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=6)
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,
)



# Define the loss function
cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction="none")

# EarlyStopping criteria
early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)


# Learning Rate Scheduler for the optimizer
class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, post_warmup_learning_rate, warmup_steps):
        super().__init__()
        self.post_warmup_learning_rate = post_warmup_learning_rate
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        global_step = tf.cast(step, tf.float32)
        warmup_steps = tf.cast(self.warmup_steps, tf.float32)
        warmup_progress = global_step / warmup_steps
        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress
        return tf.cond(
            global_step < warmup_steps,
            lambda: warmup_learning_rate,
            lambda: self.post_warmup_learning_rate,
        )


# Create a learning rate schedule
num_train_steps = len(train_dataset) * EPOCHS
num_warmup_steps = num_train_steps //15 #15
lr_schedule = LRSchedule(post_warmup_learning_rate= 1e-4, warmup_steps=num_warmup_steps)# 1e-4

# Compile the model
#caption_model.compile(optimizer=keras.optimizers.Adamax(lr_schedule), loss=cross_entropy)#loss=cross_entropy

#caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy
caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy

# Fit the model
caption_model.fit(train_dataset,epochs=EPOCHS,validation_data=valid_dataset,callbacks=[early_stopping])



# Compute definitive metrics on train/valid set
train_metrics = caption_model.evaluate(train_dataset, batch_size=BATCH_SIZE)
valid_metrics = caption_model.evaluate(valid_dataset, batch_size=BATCH_SIZE)

test_metrics = caption_model.evaluate(test_dataset, batch_size=BATCH_SIZE)

print("Train Loss = %.4f - Train Accuracy = %.4f" % (train_metrics[0], train_metrics[1]))
print("Valid Loss = %.4f - Valid Accuracy = %.4f" % (valid_metrics[0], valid_metrics[1]))

print("Test Loss = %.4f - Test Accuracy = %.4f" % (test_metrics[0], test_metrics[1]))

caption_model.save_weights('/saved model/EfficientNetV2B0/')
print('saved done!')
caption_model.load_weights('/saved model /EfficientNetV2B0/')

"""# **prediction testing belue**"""

from tqdm import tqdm
"""
vocab = vectorization.get_vocabulary()
index_lookup = dict(zip(range(len(vocab)), vocab))
max_decoded_sentence_length = SEQ_LENGTH - 1
valid_images = list(valid_data.keys())
"""

def generate_caption_noimg(data12):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in tqdm(data12):

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      for i in range(max_decoded_sentence_length):
          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token = index_lookup[sampled_token_index]
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)

def chop_start_end(cpt):
    if cpt.startswith('<start>') and cpt.endswith('<end>'):
        cpt = ' '.join(cpt.split()[1:-1])
    return cpt

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def BLEU_corpus(actual_captions, generated_captions):
    list_of_references = []
    for images_captions in actual_captions:
        list_of_references.append([chop_start_end(cpt).split() for cpt in images_captions])
    hypotheses = [chop_start_end(cpt).split() for cpt in generated_captions]
    b1 = corpus_bleu(list_of_references, hypotheses, weights=(1, 0, 0, 0))
    b2 = corpus_bleu(list_of_references, hypotheses, weights=(0.5, 0.5, 0, 0))
    b3 = corpus_bleu(list_of_references, hypotheses, weights=(0.333, 0.333, 0.333, 0))
    b4 = corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))
    return b1, b2, b3, b4

def plot_bleu_bar_graph(bleu_train, bleu_test):
    n_groups = 4
    #bleu_train = (b1_train,b2_train,b3_train,b4_train)
    #bleu_test = (b1_test,b2_test,b3_test,b4_test)
    # create plot
    fig, ax = plt.subplots()
    index = np.arange(n_groups)
    bar_width = 0.3
    opacity = 0.8

    rects1 = plt.bar(index, bleu_train, bar_width,
                     alpha=opacity,
                     color='b',
                     label='train',
                     zorder=3)

    rects2 = plt.bar(index + bar_width, bleu_test, bar_width,
                     alpha=opacity,
                     color='r',
                     label='test',
                     zorder=3)

    plt.xlabel('BLEU')
    plt.ylabel('score')
    #plt.title('Scores')
    plt.xticks(index + bar_width, ('1', '2', '3', '4'))
    plt.legend()
    plt.tight_layout()
    plt.grid(True, axis='y')
    plt.savefig("BLEU_bar.png", dpi=200)
    plt.show()

testing_data=test_data
#test_dataset
print("Number of testing samples: ", len(testing_data))

all_gen_test=generate_caption_noimg(list(testing_data.keys()))
all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])
all_gen_test
for i in range(len(all_gen_test)):
  all_gen_test[i]='<start> '+ all_gen_test[i]+ ' <end>'
b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, all_gen_test)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

"""# **beam search**"""

#sampled_token_index11=[]
def generate_caption_noimg(data12,caption_model):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in data12:

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      start_token = "<start>"
      end_token = "<end>"
      #start = [wordtoix[start_token]]

      #start_word = [[start, 0.0]]


      for i in (range(max_decoded_sentence_length)):

          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token_index11.append(predictions[0, i, :])
          #print('sampled_token_index:',sampled_token_index)
          sampled_token = index_lookup[sampled_token_index]
          #print('sampled_token:',sampled_token)
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)
from math import log
from numpy import array
from numpy import argmax

# beam search
def beam_search_decoder(data, k):
	sequences = [[list(), 0.0]]
	# walk over each step in sequence
	for row in data:
		all_candidates = list()
		# expand each current candidate
		for i in range(len(sequences)):
			seq, score = sequences[i]
			for j in range(len(row)):
				candidate = [seq + [j], score - log(row[j])]
				all_candidates.append(candidate)


		# order all candidates by score
		ordered = sorted(all_candidates, key=lambda tup:tup[1])
		# select k best
		sequences = ordered[:k]
	return sequences

# define a sequence of 10 words over a vocab of 5 words

#greedy_result=(generate_caption_noimg([list(testing_data.keys())[3],caption_model))
#beam_search_result=beam_search_decoder(np.array(sampled_token_index11),5)

all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])

sampled_token_index11=[]
final_caption_using_beam_search_and_bleu_score=[]
list_test=list(testing_data.keys())

for k in tqdm(range (len(list_test))):
  greedy_result=(generate_caption_noimg([list_test[k]],caption_model))
  #print('greedy_result:',greedy_result)
  beam_search_result=beam_search_decoder(np.array(sampled_token_index11),10)
  #print(beam_search_result)


  caption_gen_test=[]
  for i in beam_search_result:
    text=''
    for j in i[0]:
      if j!=4:
        text=text+' '+index_lookup[j]
    #print('<start> '+text+ ' <end>')
    caption_gen_test.append('<start> '+text+ ' <end>')
  #print(caption_gen_test)
  b1=[]

  for caption in caption_gen_test:
    b1_test,b2_test,b3_test,b4_test = BLEU_corpus([all_true_test[k]], [caption])
    b1.append(b1_test)

  final_caption_using_beam_search_and_bleu_score.append(caption_gen_test[b1.index(max(b1))])
  #print(final_caption_using_beam_search_and_bleu_score)
    #print('##########################################################')

  sampled_token_index11=[]

for i in range(len(final_caption_using_beam_search_and_bleu_score)):
  final_caption_using_beam_search_and_bleu_score[i]=str(final_caption_using_beam_search_and_bleu_score[i]).replace('<end>','').strip()+' <end>'

import re
for i in range(len(final_caption_using_beam_search_and_bleu_score)):

  final_caption_using_beam_search_and_bleu_score[i]=re.sub(' +', ' ',final_caption_using_beam_search_and_bleu_score[i])

b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, final_caption_using_beam_search_and_bleu_score)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

data=pd.DataFrame(zip(all_true_test,all_gen_test,final_caption_using_beam_search_and_bleu_score),columns=['all_true_test','all_gen_test','final_caption_using_beam_search_with_k_10_and_bleu_score'])

data.to_excel('/save excels/EfficientNetV2B0.xlsx')

"""scoers"""

!pip install pycocoevalcap
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice
import json


def bleu(gts,res):
    scorer = Bleu(n=4)
    # scorer += (hypo[0], ref1)   # hypo[0] = 'word1 word2 word3 ...'
    #                                 # ref = ['word1 word2 word3 ...', 'word1 word2 word3 ...']
    score, scores = scorer.compute_score(gts, res)
    print('belu = %s' % score)
    return score

def cider(gts,res):
    scorer = Cider()
    # scorer += (hypo[0], ref1)
    (score, scores) = scorer.compute_score(gts, res)
    print('cider = %s' % score)
    return score

def meteor(gts,res):
    scorer = Meteor()
    score, scores = scorer.compute_score(gts, res)
    print('meter = %s' % score)
    return score

def rouge(gts,res):
    scorer = Rouge()
    score, scores = scorer.compute_score(gts, res)
    print('rouge = %s' % score)
    return score

def spice(gts,res):
    scorer = Spice()
    score, scores = scorer.compute_score(gts, res)
    print('spice = %s' % score)
    return score

def main(gts,res):
    b1,b2,b3,b4=bleu(gts,res)
    cid=cider(gts,res)
    met=meteor(gts,res)
    rou=rouge(gts,res)
    spi=spice(gts,res)
    return b1,b2,b3,b4,cid,met,rou,spi

#score ith greedy search

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(all_gen_test[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)

#score with beam search k=10

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(final_caption_using_beam_search_and_bleu_score[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)



def get_cnn_model():
    base_model =   resnet.ResNet152(
        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights="imagenet",
    )
    # We freeze our feature extractor
    base_model.trainable = False
    base_model_out = base_model.output
    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)
    cnn_model = keras.models.Model(base_model.input, base_model_out)
    return cnn_model

class TransformerEncoderBlock(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.0
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.dense_1 = layers.Dense(embed_dim, activation="relu")

    def call(self, inputs, training, mask=None):
        inputs = self.layernorm_1(inputs)
        inputs = self.dense_1(inputs)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=None,
            training=training,
        )
        out_1 = self.layernorm_2(inputs + attention_output_1)
        return out_1


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_tokens = embedded_tokens * self.embed_scale
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)


class TransformerDecoderBlock(layers.Layer):
    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.3
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.2
        )
        self.ffn_layer_1 = layers.Dense(ff_dim, activation="relu")
        self.ffn_layer_2 = layers.Dense(embed_dim)

        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()

        self.embedding = PositionalEmbedding(
            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE
        )
        self.out = layers.Dense(VOCAB_SIZE, activation="softmax")

        self.dropout_1 = layers.Dropout(0.3)#0.1
        self.dropout_2 = layers.Dropout(0.5)#0.1
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, training, mask=None):
        inputs = self.embedding(inputs)
        causal_mask = self.get_causal_attention_mask(inputs)

        if mask is not None:
            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)
            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
            combined_mask = tf.minimum(combined_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=combined_mask,
            training=training,
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
            training=training,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        ffn_out = self.ffn_layer_1(out_2)
        ffn_out = self.dropout_1(ffn_out, training=training)
        ffn_out = self.ffn_layer_2(ffn_out)

        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)
        ffn_out = self.dropout_2(ffn_out, training=training)
        preds = self.out(ffn_out)
        return preds

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)


class ImageCaptioningModel(keras.Model):
    def __init__(
        self, cnn_model, encoder, decoder, num_captions_per_image=3, image_aug=None,
    ):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.acc_tracker = keras.metrics.Mean(name="accuracy")
        self.num_captions_per_image = num_captions_per_image
        self.image_aug = image_aug

    def calculate_loss(self, y_true, y_pred, mask):
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)

    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        accuracy = tf.math.logical_and(mask, accuracy)
        accuracy = tf.cast(accuracy, dtype=tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)

    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):
        encoder_out = self.encoder(img_embed, training=training)
        batch_seq_inp = batch_seq[:, :-1]
        batch_seq_true = batch_seq[:, 1:]
        mask = tf.math.not_equal(batch_seq_true, 0)
        batch_seq_pred = self.decoder(
            batch_seq_inp, encoder_out, training=training, mask=mask
        )
        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)
        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)
        return loss, acc

    def train_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        if self.image_aug:
            batch_img = self.image_aug(batch_img)

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            with tf.GradientTape() as tape:
                loss, acc = self._compute_caption_loss_and_acc(
                    img_embed, batch_seq[:, i, :], training=True
                )

                # 3. Update loss and accuracy
                batch_loss += loss
                batch_acc += acc

            # 4. Get the list of all the trainable weights
            train_vars = (
                self.encoder.trainable_variables + self.decoder.trainable_variables
            )

            # 5. Get the gradients
            grads = tape.gradient(loss, train_vars)

            # 6. Update the trainable weights
            self.optimizer.apply_gradients(zip(grads, train_vars))

        # 7. Update the trackers
        batch_acc /= float(self.num_captions_per_image)
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 8. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    def test_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            loss, acc = self._compute_caption_loss_and_acc(
                img_embed, batch_seq[:, i, :], training=False
            )

            # 3. Update batch loss and batch accuracy
            batch_loss += loss
            batch_acc += acc

        batch_acc /= float(self.num_captions_per_image)

        # 4. Update the trackers
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 5. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(), "acc": self.acc_tracker.result()}

    @property
    def metrics(self):
        # We need to list our metrics here so the `reset_states()` can be
        # called automatically.
        return [self.loss_tracker, self.acc_tracker]


cnn_model = get_cnn_model()
encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=5)
decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=6)
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,
)

# Define the loss function
cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction="none")

# EarlyStopping criteria
early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)


# Learning Rate Scheduler for the optimizer
class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, post_warmup_learning_rate, warmup_steps):
        super().__init__()
        self.post_warmup_learning_rate = post_warmup_learning_rate
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        global_step = tf.cast(step, tf.float32)
        warmup_steps = tf.cast(self.warmup_steps, tf.float32)
        warmup_progress = global_step / warmup_steps
        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress
        return tf.cond(
            global_step < warmup_steps,
            lambda: warmup_learning_rate,
            lambda: self.post_warmup_learning_rate,
        )


# Create a learning rate schedule
num_train_steps = len(train_dataset) * EPOCHS
num_warmup_steps = num_train_steps //15 #15
lr_schedule = LRSchedule(post_warmup_learning_rate= 1e-4, warmup_steps=num_warmup_steps)# 1e-4

# Compile the model
#caption_model.compile(optimizer=keras.optimizers.Adamax(lr_schedule), loss=cross_entropy)#loss=cross_entropy

#caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy
caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)#loss=cross_entropy

# Fit the model
caption_model.fit(train_dataset,epochs=EPOCHS,validation_data=valid_dataset,callbacks=[early_stopping])

# Compute definitive metrics on train/valid set
train_metrics = caption_model.evaluate(train_dataset, batch_size=BATCH_SIZE)
valid_metrics = caption_model.evaluate(valid_dataset, batch_size=BATCH_SIZE)

test_metrics = caption_model.evaluate(test_dataset, batch_size=BATCH_SIZE)

print("Train Loss = %.4f - Train Accuracy = %.4f" % (train_metrics[0], train_metrics[1]))
print("Valid Loss = %.4f - Valid Accuracy = %.4f" % (valid_metrics[0], valid_metrics[1]))

print("Test Loss = %.4f - Test Accuracy = %.4f" % (test_metrics[0], test_metrics[1]))

caption_model.save_weights('/saved model/ ResNet152/')
print('saved done!')
caption_model.load_weights('/saved model /ResNet152/')



from tqdm import tqdm
"""
vocab = vectorization.get_vocabulary()
index_lookup = dict(zip(range(len(vocab)), vocab))
max_decoded_sentence_length = SEQ_LENGTH - 1
valid_images = list(valid_data.keys())
"""

def generate_caption_noimg(data12):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in tqdm(data12):

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      for i in range(max_decoded_sentence_length):
          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token = index_lookup[sampled_token_index]
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)

def chop_start_end(cpt):
    if cpt.startswith('<start>') and cpt.endswith('<end>'):
        cpt = ' '.join(cpt.split()[1:-1])
    return cpt

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def BLEU_corpus(actual_captions, generated_captions):
    list_of_references = []
    for images_captions in actual_captions:
        list_of_references.append([chop_start_end(cpt).split() for cpt in images_captions])
    hypotheses = [chop_start_end(cpt).split() for cpt in generated_captions]
    b1 = corpus_bleu(list_of_references, hypotheses, weights=(1, 0, 0, 0))
    b2 = corpus_bleu(list_of_references, hypotheses, weights=(0.5, 0.5, 0, 0))
    b3 = corpus_bleu(list_of_references, hypotheses, weights=(0.333, 0.333, 0.333, 0))
    b4 = corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))
    return b1, b2, b3, b4

def plot_bleu_bar_graph(bleu_train, bleu_test):
    n_groups = 4
    #bleu_train = (b1_train,b2_train,b3_train,b4_train)
    #bleu_test = (b1_test,b2_test,b3_test,b4_test)
    # create plot
    fig, ax = plt.subplots()
    index = np.arange(n_groups)
    bar_width = 0.3
    opacity = 0.8

    rects1 = plt.bar(index, bleu_train, bar_width,
                     alpha=opacity,
                     color='b',
                     label='train',
                     zorder=3)

    rects2 = plt.bar(index + bar_width, bleu_test, bar_width,
                     alpha=opacity,
                     color='r',
                     label='test',
                     zorder=3)

    plt.xlabel('BLEU')
    plt.ylabel('score')
    #plt.title('Scores')
    plt.xticks(index + bar_width, ('1', '2', '3', '4'))
    plt.legend()
    plt.tight_layout()
    plt.grid(True, axis='y')
    plt.savefig("BLEU_bar.png", dpi=200)
    plt.show()

testing_data=test_data
#test_dataset
print("Number of testing samples: ", len(testing_data))

all_gen_test=generate_caption_noimg(list(testing_data.keys()))
all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])
all_gen_test
for i in range(len(all_gen_test)):
  all_gen_test[i]='<start> '+ all_gen_test[i]+ ' <end>'
b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, all_gen_test)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

"""# **beam search**"""

#sampled_token_index11=[]
def generate_caption_noimg(data12,caption_model):
    # Select a random image from the validation dataset
    # sample_img = np.random.choice(valid_images)
    all=[]
    for sample_img in data12:

      # Read the image from the disk
      sample_img = decode_and_resize(sample_img)
      img = sample_img.numpy().clip(0, 255).astype(np.uint8)
     # plt.imshow(img)
      #plt.show()

      # Pass the image to the CNN
      img = tf.expand_dims(sample_img, 0)
      img = caption_model.cnn_model(img)

      # Pass the image features to the Transformer encoder
      encoded_img = caption_model.encoder(img, training=False)

      # Generate the caption using the Transformer decoder
      decoded_caption = "<start> "
      start_token = "<start>"
      end_token = "<end>"
      #start = [wordtoix[start_token]]

      #start_word = [[start, 0.0]]


      for i in (range(max_decoded_sentence_length)):

          tokenized_caption = vectorization([decoded_caption])[:, :-1]
          mask = tf.math.not_equal(tokenized_caption, 0)
          predictions = caption_model.decoder(
              tokenized_caption, encoded_img, training=False, mask=mask
          )
          sampled_token_index = np.argmax(predictions[0, i, :])
          sampled_token_index11.append(predictions[0, i, :])
          #print('sampled_token_index:',sampled_token_index)
          sampled_token = index_lookup[sampled_token_index]
          #print('sampled_token:',sampled_token)
          if sampled_token == " <end>":
              break
          decoded_caption += " " + sampled_token

      decoded_caption = decoded_caption.replace("<start> ", "")
      decoded_caption = decoded_caption.replace(" <end>", "").strip()
      all.append(decoded_caption)
    return all
      # print("Predicted Caption: ", decoded_caption)
from math import log
from numpy import array
from numpy import argmax

# beam search
def beam_search_decoder(data, k):
	sequences = [[list(), 0.0]]
	# walk over each step in sequence
	for row in data:
		all_candidates = list()
		# expand each current candidate
		for i in range(len(sequences)):
			seq, score = sequences[i]
			for j in range(len(row)):
				candidate = [seq + [j], score - log(row[j])]
				all_candidates.append(candidate)


		# order all candidates by score
		ordered = sorted(all_candidates, key=lambda tup:tup[1])
		# select k best
		sequences = ordered[:k]
	return sequences

# define a sequence of 10 words over a vocab of 5 words

#greedy_result=(generate_caption_noimg([list(testing_data.keys())[3],caption_model))
#beam_search_result=beam_search_decoder(np.array(sampled_token_index11),5)

all_true_test=[]
for key in testing_data.keys():
  all_true_test.append(testing_data[key])

sampled_token_index11=[]
final_caption_using_beam_search_and_bleu_score=[]
list_test=list(testing_data.keys())

for k in tqdm(range (len(list_test))):
  greedy_result=(generate_caption_noimg([list_test[k]],caption_model))
  #print('greedy_result:',greedy_result)
  beam_search_result=beam_search_decoder(np.array(sampled_token_index11),10)
  #print(beam_search_result)


  caption_gen_test=[]
  for i in beam_search_result:
    text=''
    for j in i[0]:
      if j!=4:
        text=text+' '+index_lookup[j]
    #print('<start> '+text+ ' <end>')
    caption_gen_test.append('<start> '+text+ ' <end>')
  #print(caption_gen_test)
  b1=[]

  for caption in caption_gen_test:
    b1_test,b2_test,b3_test,b4_test = BLEU_corpus([all_true_test[k]], [caption])
    b1.append(b1_test)

  final_caption_using_beam_search_and_bleu_score.append(caption_gen_test[b1.index(max(b1))])
  #print(final_caption_using_beam_search_and_bleu_score)
    #print('##########################################################')

  sampled_token_index11=[]

for i in range(len(final_caption_using_beam_search_and_bleu_score)):
  final_caption_using_beam_search_and_bleu_score[i]=str(final_caption_using_beam_search_and_bleu_score[i]).replace('<end>','').strip()+' <end>'

import re
for i in range(len(final_caption_using_beam_search_and_bleu_score)):

  final_caption_using_beam_search_and_bleu_score[i]=re.sub(' +', ' ',final_caption_using_beam_search_and_bleu_score[i])

b1_test,b2_test,b3_test,b4_test = BLEU_corpus(all_true_test, final_caption_using_beam_search_and_bleu_score)
print('b1_test=',b1_test)
print('b2_test=',b2_test)
print('b3_test=',b3_test)
print('b4_test=',b4_test)

data=pd.DataFrame(zip(all_true_test,all_gen_test,final_caption_using_beam_search_and_bleu_score),columns=['all_true_test','all_gen_test','final_caption_using_beam_search_with_k_10_and_bleu_score'])

data.to_excel('/save excels/ResNet152.xlsx')

!pip install pycocoevalcap
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice
import json


def bleu(gts,res):
    scorer = Bleu(n=4)
    # scorer += (hypo[0], ref1)   # hypo[0] = 'word1 word2 word3 ...'
    #                                 # ref = ['word1 word2 word3 ...', 'word1 word2 word3 ...']
    score, scores = scorer.compute_score(gts, res)
    print('belu = %s' % score)
    return score

def cider(gts,res):
    scorer = Cider()
    # scorer += (hypo[0], ref1)
    (score, scores) = scorer.compute_score(gts, res)
    print('cider = %s' % score)
    return score

def meteor(gts,res):
    scorer = Meteor()
    score, scores = scorer.compute_score(gts, res)
    print('meter = %s' % score)
    return score

def rouge(gts,res):
    scorer = Rouge()
    score, scores = scorer.compute_score(gts, res)
    print('rouge = %s' % score)
    return score

def spice(gts,res):
    scorer = Spice()
    score, scores = scorer.compute_score(gts, res)
    print('spice = %s' % score)
    return score

def main(gts,res):
    b1,b2,b3,b4=bleu(gts,res)
    cid=cider(gts,res)
    met=meteor(gts,res)
    rou=rouge(gts,res)
    spi=spice(gts,res)
    return b1,b2,b3,b4,cid,met,rou,spi

#score ith greedy search

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(all_gen_test[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)

#score with beam search k=10

references_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))
  references_test[key]=temp


hypoth_test={}
for i,key in tqdm(enumerate(test_data.keys())):
  hypoth_test[key]=[chop_start_end(final_caption_using_beam_search_and_bleu_score[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)



def chop_start_end(cpt):
    if cpt.startswith('<start>') and cpt.endswith('<end>'):
        cpt = ' '.join(cpt.split()[1:-1])
    return cpt

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def BLEU_corpus(actual_captions, generated_captions):
    list_of_references = []
    for images_captions in actual_captions:
        list_of_references.append([chop_start_end(cpt).split() for cpt in images_captions])
    hypotheses = [chop_start_end(cpt).split() for cpt in generated_captions]
    b1 = corpus_bleu(list_of_references, hypotheses, weights=(1, 0, 0, 0))
    b2 = corpus_bleu(list_of_references, hypotheses, weights=(0.5, 0.5, 0, 0))
    b3 = corpus_bleu(list_of_references, hypotheses, weights=(0.333, 0.333, 0.333, 0))
    b4 = corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))
    return b1, b2, b3, b4



def plot_bleu_bar_graph(bleu_train, bleu_test):
    n_groups = 4
    #bleu_train = (b1_train,b2_train,b3_train,b4_train)
    #bleu_test = (b1_test,b2_test,b3_test,b4_test)
    # create plot
    fig, ax = plt.subplots()
    index = np.arange(n_groups)
    bar_width = 0.3
    opacity = 0.8

    rects1 = plt.bar(index, bleu_train, bar_width,
                     alpha=opacity,
                     color='b',
                     label='train',
                     zorder=3)

    rects2 = plt.bar(index + bar_width, bleu_test, bar_width,
                     alpha=opacity,
                     color='r',
                     label='test',
                     zorder=3)

    plt.xlabel('BLEU')
    plt.ylabel('score')
    #plt.title('Scores')
    plt.xticks(index + bar_width, ('1', '2', '3', '4'))
    plt.legend()
    plt.tight_layout()
    plt.grid(True, axis='y')
    plt.savefig("BLEU_bar.png", dpi=200)
    plt.show()

vgg16_all_gen_test=pd.read_excel('/vgg16.xlsx')['final_caption_using_beam_search_with_k_10_and_bleu_score'].to_list()
EfficientNetB4_all_gen_test=pd.read_excel('/ EfficientNetB4.xlsx')['final_caption_using_beam_search_with_k_10_and_bleu_score'].to_list()
ResNet152_all_gen_test=pd.read_excel('/ResNet152.xlsx')['final_caption_using_beam_search_with_k_10_and_bleu_score'].to_list()
ResNet50_all_gen_test=pd.read_excel('/ResNet50.xlsx')['final_caption_using_beam_search_with_k_10_and_bleu_score'].to_list()
EfficientNetV2B0_all_gen_test=pd.read_excel('/ EfficientNetV2B0.xlsx')['final_caption_using_beam_search_with_k_10_and_bleu_score'].to_list()
vgg19_all_gen_test=pd.read_excel('/VGG19.xlsx')['final_caption_using_beam_search_with_k_10_and_bleu_score'].to_list()
ResNet101_all_gen_test=pd.read_excel('/ResNet101.xlsx')['final_caption_using_beam_search_with_k_10_and_bleu_score'].to_list()
RegNetX120_all_gen_test=pd.read_excel('/RegNetX120.xlsx')['final_caption_using_beam_search_with_k_10_and_bleu_score'].to_list()
all_true_test=pd.read_excel('/vgg16.xlsx')
from ast import literal_eval
all_true_test['all_true_test'] = (all_true_test['all_true_test'].apply(literal_eval))
all_true_test =all_true_test['all_true_test'].to_list()

vgg16_b1_score=[]
vgg19_b1_score=[]
ResNet101_b1_score=[]
ResNet152_b1_score=[]
ResNet50_b1_score=[]
EfficientNetB4_b1_score=[]
EfficientNetV2B0_b1_score=[]
RegNetX120_b1_score=[]



!pip install pycocoevalcap
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice
import json


def bleu(gts,res):
    scorer = Bleu(n=4)
    # scorer += (hypo[0], ref1)   # hypo[0] = 'word1 word2 word3 ...'
    #                                 # ref = ['word1 word2 word3 ...', 'word1 word2 word3 ...']
    score, scores = scorer.compute_score(gts, res)
    print('belu = %s' % score)
    return score

def cider(gts,res):
    scorer = Cider()
    # scorer += (hypo[0], ref1)
    (score, scores) = scorer.compute_score(gts, res)
    print('cider = %s' % score)
    return score

def meteor(gts,res):
    scorer = Meteor()
    score, scores = scorer.compute_score(gts, res)
    print('meter = %s' % score)
    return score

def rouge(gts,res):
    scorer = Rouge()
    score, scores = scorer.compute_score(gts, res)
    print('rouge = %s' % score)
    return score

def spice(gts,res):
    scorer = Spice()
    score, scores = scorer.compute_score(gts, res)
    print('spice = %s' % score)
    return score

def main(gts,res):
    b1,b2,b3,b4=bleu(gts,res)
    cid=cider(gts,res)
    met=meteor(gts,res)
    rou=rouge(gts,res)
    spi=spice(gts,res)
    return b1,b2,b3,b4,cid,met,rou,spi

#score ith greedy search

references_test={}
for i,key in tqdm(enumerate(all_true_test)):
  temp=[]
  for j in range(len(all_true_test[i])):
    temp.append(chop_start_end(all_true_test[i][j]))

  references_test[key[0]]=temp



hypoth_test={}
for i,key in tqdm(enumerate(all_true_test)):
  hypoth_test[key[0]]=[chop_start_end(final_caption_using_voting[i])]


b1_te,b2_te,b3_te,b4_te,cid_te,met_te,rou_te,spi_te=main(references_test,hypoth_test)
